{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_groupchat_compression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Generated Agent Chat: Group Chat with CompressibleGroupChatManager (Experimental)\n",
    "\n",
    "AutoGen offers conversable agents powered by LLM, tool or human, which can be used to perform tasks collectively via automated chat. This framework allows tool use and human participation through multi-agent conversation.\n",
    "Please find documentation about this feature [here](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat).\n",
    "\n",
    "In this notebook, we demonstrate how to use an experimental version of AutoGen's GroupChatManager, `CompressibleGroupChatManager`, to compress the conversation history.\n",
    "- If `compress_config=False`, the `CompressibleGroupChatManager` is equivalent to `GroupChatManager`. \n",
    "- If turned on, it will perform compression when a pre-set token count is reached. The compressed messages will be broadcasted to all agents in the conversation.\n",
    "\n",
    "There are different modes to be used: `TERMINATE, COMPRESS, CUMSTOMIZED`. You can read more about compression and compressible agents in `agentchat_compression.ipynb`. \n",
    "\n",
    "## Requirements\n",
    "\n",
    "AutoGen requires `Python>=3.8`. To run this notebook example, please install:\n",
    "```bash\n",
    "pip install pyautogen\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "# %pip install pyautogen~=0.1.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "The [`config_list_from_json`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "config_list_gpt4 = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4\", \"gpt-4-0314\", \"gpt4\", \"gpt-4-32k\", \"gpt-4-32k-0314\", \"gpt-4-32k-v0314\"],\n",
    "    },\n",
    ")\n",
    "# config_list_gpt35 = autogen.config_list_from_json(\n",
    "#     \"OAI_CONFIG_LIST\",\n",
    "#     filter_dict={\n",
    "#         \"model\": {\n",
    "#             \"gpt-3.5-turbo\",\n",
    "#             \"gpt-3.5-turbo-16k\",\n",
    "#             \"gpt-3.5-turbo-0301\",\n",
    "#             \"chatgpt-35-turbo-0301\",\n",
    "#             \"gpt-35-turbo-v0301\",\n",
    "#         },\n",
    "#     },\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It first looks for environment variable \"OAI_CONFIG_LIST\" which needs to be a valid json string. If that variable is not found, it then looks for a json file named \"OAI_CONFIG_LIST\". It filters the configs by models (you can filter by other keys as well). Only the gpt-4 models are kept in the list based on the filter condition.\n",
    "\n",
    "The config list looks like the following:\n",
    "```python\n",
    "config_list = [\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your OpenAI API key here>',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'base_url': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-4-32k',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'base_url': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "]\n",
    "```\n",
    "\n",
    "If you open this notebook in colab, you can upload your files by clicking the file icon on the left panel and then choose \"upload file\" icon.\n",
    "\n",
    "You can set the value of config_list in other ways you prefer, e.g., loading from a YAML file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example group chat with compression\n",
    "\n",
    "This example is from [agentchat_groupchat.ipynb](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_groupchat.ipynb). \n",
    "\n",
    "Simply replace `GroupChatManager` with `CompressibleGroupChatManager` to enable compression. Refer to `agentchat_compression.ipynb` for other ways of handling long conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "llm_config = {\"config_list\": config_list_gpt4, \"seed\": 42}\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "   name=\"User_proxy\",\n",
    "   system_message=\"A human admin.\",\n",
    "   code_execution_config={\"last_n_messages\": 2, \"work_dir\": \"groupchat\"},\n",
    "   human_input_mode=\"TERMINATE\"\n",
    ")\n",
    "coder = autogen.AssistantAgent(\n",
    "    name=\"Coder\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "pm = autogen.AssistantAgent(\n",
    "    name=\"Product_manager\",\n",
    "    system_message=\"Creative in software product ideas.\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "groupchat = autogen.GroupChat(agents=[user_proxy, coder, pm], messages=[], max_round=20)\n",
    "\n",
    "# replace GroupChatManager with CompressibleGroupChatManager\n",
    "# manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=gpt4_config)\n",
    "from autogen.agentchat.contrib.compressible_groupchat import CompressibleGroupChatManager\n",
    "manager = CompressibleGroupChatManager(\n",
    "    groupchat=groupchat, \n",
    "    llm_config=llm_config,\n",
    "    compress_config={\n",
    "        \"mode\": \"COMPRESS\",\n",
    "        \"trigger_count\": 2000,\n",
    "        \"leave_last_n\": 2, # number of messages to consider for compression\n",
    "        \"verbose\": False, # to allow printing of compression information\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "To achieve the task, we need to perform two steps:\n",
      "\n",
      "1. We need to search the latest papers about GPT-4 on arXiv.\n",
      "2. Once we have the paper details, we need to extract information related to GPT-4's potential applications in software from it.\n",
      "\n",
      "I can help to get automated search results from arXiv using its API. However, fetching data from downloaded papers is more complex, as they are normally in PDF format. An AI model can't read and comprehend text from a PDF in the same way a human would, but I can assist you to get the paper, you would need to manually read it for detailed information.\n",
      "\n",
      "Let's start with the first step, which is to find the latest research paper about GPT-4 on arXiv. We will use the `arxiv` python library for this, which is a Python wrapper for the arXiv API. Follow along with this python code.\n",
      "\n",
      "Note: You may need to install the `arxiv` library if it's not already installed on your system. You can do that by running `pip install arxiv`.\n",
      "\n",
      "```python\n",
      "# import required library\n",
      "import arxiv\n",
      "\n",
      "# search for the latest papers on GPT-4\n",
      "search = arxiv.Search(\n",
      "  query = \"GPT-4\",\n",
      "  max_results = 1,\n",
      "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
      ")\n",
      "\n",
      "# print the latest paper details\n",
      "for result in search.get():\n",
      "  print(f\"Title: {result.title}\")\n",
      "  print(f\"Author(s): {', '.join(result.authors)}\")\n",
      "  print(f\"Abstract: {result.summary}\")\n",
      "  print(f\"Link: {result.pdf_url}\")\n",
      "```\n",
      "\n",
      "Once you get the paper details, you can manually check the paper for potential applications of GPT-4 in software.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "Traceback (most recent call last):\n",
      "  File \"\", line 12, in <module>\n",
      "    for result in search.get():\n",
      "AttributeError: 'Search' object has no attribute 'get'\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "I apologize for the confusion. The `Search` object indeed doesn't have a `get()` method. We should iterate over the `Search` object directly.\n",
      "\n",
      "Directly iterating over the `Search` object will give the desired result. This is because the `Search` object is an iterable and it uses `get()` method internally.\n",
      "\n",
      "Here is the corrected Python code:\n",
      "\n",
      "```python\n",
      "# import required library\n",
      "import arxiv\n",
      "\n",
      "# search for the latest papers on GPT-4\n",
      "search = arxiv.Search(\n",
      "  query = \"GPT-4\",\n",
      "  max_results = 1,\n",
      "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
      ")\n",
      "\n",
      "# print the latest paper details\n",
      "for result in search.results():\n",
      "  print(f\"Title: {result.title}\")\n",
      "  print(f\"Author(s): {', '.join(result.authors)}\")\n",
      "  print(f\"Abstract: {result.summary}\")\n",
      "  print(f\"Link: {result.pdf_url}\")\n",
      "```\n",
      "\n",
      "This will correctly print the details of the most recent paper on GPT-4 as per the arXiv API results.\n",
      "\n",
      "Please run the corrected code and let me know the output.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      ":12: DeprecationWarning: The '(Search).results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n",
      "Traceback (most recent call last):\n",
      "  File \"\", line 14, in <module>\n",
      "    print(f\"Author(s): {', '.join(result.authors)}\")\n",
      "TypeError: sequence item 0: expected str instance, Author found\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "I apologize for the confusion again. It seems there has been some changes in the way authors are represented in the latest versions of the arxiv library. The authors are now returned as `Author` objects.\n",
      "\n",
      "Let's modify the code to handle this:\n",
      "\n",
      "```python\n",
      "# import required library\n",
      "import arxiv\n",
      "\n",
      "# search for the latest papers on GPT-4\n",
      "search = arxiv.Search(\n",
      "  query = \"GPT-4\",\n",
      "  max_results = 1,\n",
      "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
      ")\n",
      "\n",
      "# print the latest paper details\n",
      "for result in search.get():\n",
      "  print(f\"Title: {result.title}\")\n",
      "\n",
      "  # Build author list as string\n",
      "  authors = \", \".join([str(author) for author in result.authors])\n",
      "  print(f\"Author(s): {authors}\")\n",
      "\n",
      "  print(f\"Abstract: {result.summary}\")\n",
      "  print(f\"Link: {result.pdf_url}\")\n",
      "```\n",
      "\n",
      "Please run the corrected code and let me know the output.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "Traceback (most recent call last):\n",
      "  File \"\", line 12, in <module>\n",
      "    for result in search.get():\n",
      "AttributeError: 'Search' object has no attribute 'get'\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "I apologize for the confusion earlier. The 'get()' method was inadvertently left in the code. In fact, we need to iterate over the `Search` object directly. This required change has been reflected in the code provided below.\n",
      "\n",
      "```python\n",
      "# import required library\n",
      "import arxiv\n",
      "\n",
      "# search for the latest papers on GPT-4\n",
      "search = arxiv.Search(\n",
      "  query = \"GPT-4\",\n",
      "  max_results = 1,\n",
      "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
      ")\n",
      "\n",
      "# print the latest paper details\n",
      "for result in search.results():\n",
      "  print(f\"Title: {result.title}\")\n",
      "\n",
      "  # Build author list as string\n",
      "  authors = \", \".join([str(author) for author in result.authors])\n",
      "  print(f\"Author(s): {authors}\")\n",
      "\n",
      "  print(f\"Abstract: {result.summary}\")\n",
      "  print(f\"Link: {result.pdf_url}\")\n",
      "```\n",
      "\n",
      "Please run this corrected code. My apologies for any confusion caused by the previous erroneous code suggestions. I'm here to help, so please don't hesitate to bring up any additional issues if they arise.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Title: PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion\n",
      "Author(s): Yiduo Guo, Zekai Zhang, Yaobo Liang, Dongyan Zhao, Duan Nan\n",
      "Abstract: Recent evaluations of Large Language Models (LLMs) have centered around\n",
      "testing their zero-shot/few-shot capabilities for basic natural language tasks\n",
      "and their ability to translate instructions into tool APIs. However, the\n",
      "evaluation of LLMs utilizing complex tools to finish multi-turn, multi-modal\n",
      "instructions in a complex multi-modal environment has not been investigated. To\n",
      "address this gap, we introduce the PowerPoint Task Completion (PPTC) benchmark\n",
      "to assess LLMs' ability to create and edit PPT files based on user\n",
      "instructions. It contains 279 multi-turn sessions covering diverse topics and\n",
      "hundreds of instructions involving multi-modal operations. We also propose the\n",
      "PPTX-Match Evaluation System that evaluates if LLMs finish the instruction\n",
      "based on the prediction file rather than the label API sequence, thus it\n",
      "supports various LLM-generated API sequences. We measure 3 closed LLMs and 6\n",
      "open-source LLMs. The results show that GPT-4 outperforms other LLMs with\n",
      "75.1\\% accuracy in single-turn dialogue testing but faces challenges in\n",
      "completing entire sessions, achieving just 6\\% session accuracy. We find three\n",
      "main error causes in our benchmark: error accumulation in the multi-turn\n",
      "session, long PPT template processing, and multi-modality perception. These\n",
      "pose great challenges for future LLM and agent systems. We release the data,\n",
      "code, and evaluation system of PPTC at \\url{https://github.com/gydpku/PPTC}.\n",
      "Link: http://arxiv.org/pdf/2311.01767v1\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35mToken Count (including 530 tokens from system msg and function descriptions). Before compression : 2269 | After: 1610\u001b[0m\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mProduct_manager\u001b[0m (to chat_manager):\n",
      "\n",
      "The GPT-4 model presented in the paper titled \"PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion\" is evaluated for its ability to process multi-modal, multi-turn user instructions and implement them in the form of PowerPoint presentations. This involves creating and editing PowerPoint files based on given instructions. \n",
      "\n",
      "Here are few potential applications of GPT-4 in software based on this paper:\n",
      "\n",
      "1. **Presentation Creation Software**: GPT-4 can be integrated into presentation creation software to assist users in creating professional and effective presentations based on user instructions.\n",
      "\n",
      "2. **Educational Software**: GPT-4 can be used in educational software to create study materials, tutorials or lectures based on specific topics, making it easier for educators to prepare content.\n",
      "\n",
      "3. **Automated Report Generation**: Businesses can use GPT-4 to generate professional reports in PowerPoint format, automating a significant portion of the reporting process.\n",
      "\n",
      "4. **Interactive Software Tutorials**: GPT-4 can create interactive tutorials for software, following instructions given for each step and presenting them in a visual, comprehensible manner.\n",
      "\n",
      "5. **Collaborative Tools**: GPT-4 can be deployed in collaborative tools, taking in user instructions and simultaneously reflecting changes across an entire team's view, enhancing collaborative work experience.\n",
      "\n",
      "6. **AI Coaching Tools**: The application of GPT-4 in personal AI coaching tools where it could guide users on how to make effective PowerPoint presentations.\n",
      "\n",
      "These software integrations hinge on GPT-4's superior language understanding and generation capabilities, which can translate user instructions into precise API calls to manipulate PowerPoint files. It's important to note that such applications need to address the issues pointed out in the paper like error accumulation in multi-turn sessions and long PPT template processing.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35mToken Count (including 530 tokens from system msg and function descriptions). Before compression : 2007 | After: 1633\u001b[0m\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "user_proxy.initiate_chat(manager, message=\"Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\")\n",
    "# type exit to terminate the chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flaml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
