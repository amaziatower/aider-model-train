{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_groupchat_compression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Generated Agent Chat: Group Chat with CompressibleGroupChatManager (Experimental)\n",
    "\n",
    "AutoGen offers conversable agents powered by LLM, tools, or humans, which can be used to perform tasks collectively via automated chat. This framework allows tool use and human participation through multi-agent conversation.\n",
    "Please find documentation about this feature [here](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat).\n",
    "\n",
    "In this notebook, we demonstrate how to use an experimental version of AutoGen's GroupChatManager, `CompressibleGroupChatManager`. If compression is turned on, the manager will check the token limit before selecting the next agent to speak and update the chat history if the limit is reached. The updated messages will be broadcast to all agents in the conversation. To use the compression feature, simply replace `GroupChatManager` with `CompressibleGroupChatManager`, and set the `compress_config`. There are different modes to be used: `TERMINATE, COMPRESS, CUSTOM`. The compressible manager inherits from `CompressibleAgent`, and you can read more about compression and compressible agents in `agentchat_compression.ipynb`.\n",
    "\n",
    "Note: The trigger amount is compared with token count of the groupchatmanager, which will have a system message aggregated from all agents in the groupchat. This doesn't matter what groupchat selection strategy is used (manual, roundrobin, etc). This will be improved in the future.\n",
    "You can print `groupchatmanager.init_count` to get the initial token count.\n",
    "\n",
    "\n",
    "## Requirements\n",
    "\n",
    "AutoGen requires `Python>=3.8`. To run this notebook example, please install:\n",
    "```bash\n",
    "pip install pyautogen\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "# %pip install pyautogen~=0.1.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "The [`config_list_from_json`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "config_list_gpt4 = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4\", \"gpt-4-0314\", \"gpt4\", \"gpt-4-32k\", \"gpt-4-32k-0314\", \"gpt-4-32k-v0314\"],\n",
    "    },\n",
    ")\n",
    "# config_list_gpt35 = autogen.config_list_from_json(\n",
    "#     \"OAI_CONFIG_LIST\",\n",
    "#     filter_dict={\n",
    "#         \"model\": {\n",
    "#             \"gpt-3.5-turbo\",\n",
    "#             \"gpt-3.5-turbo-16k\",\n",
    "#             \"gpt-3.5-turbo-0301\",\n",
    "#             \"chatgpt-35-turbo-0301\",\n",
    "#             \"gpt-35-turbo-v0301\",\n",
    "#         },\n",
    "#     },\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It first looks for environment variable \"OAI_CONFIG_LIST\" which needs to be a valid json string. If that variable is not found, it then looks for a json file named \"OAI_CONFIG_LIST\". It filters the configs by models (you can filter by other keys as well). Only the gpt-4 models are kept in the list based on the filter condition.\n",
    "\n",
    "The config list looks like the following:\n",
    "```python\n",
    "config_list = [\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your OpenAI API key here>',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'base_url': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-4-32k',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'base_url': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "]\n",
    "```\n",
    "\n",
    "If you open this notebook in colab, you can upload your files by clicking the file icon on the left panel and then choose \"upload file\" icon.\n",
    "\n",
    "You can set the value of config_list in other ways you prefer, e.g., loading from a YAML file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example group chat with compression\n",
    "\n",
    "This example is from [agentchat_groupchat.ipynb](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_groupchat.ipynb). \n",
    "\n",
    "Simply replace `GroupChatManager` with `CompressibleGroupChatManager` to enable compression. Refer to `agentchat_compression.ipynb` for other ways of handling long conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial token count from manager: 567\n"
     ]
    }
   ],
   "source": [
    "import autogen\n",
    "llm_config = {\"config_list\": config_list_gpt4, \"cache_seed\": 42}\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "   name=\"User_proxy\",\n",
    "   system_message=\"A human admin.\",\n",
    "   code_execution_config={\"last_n_messages\": 2, \"work_dir\": \"groupchat\"},\n",
    "   human_input_mode=\"TERMINATE\"\n",
    ")\n",
    "coder = autogen.AssistantAgent(\n",
    "    name=\"Coder\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "pm = autogen.AssistantAgent(\n",
    "    name=\"Product_manager\",\n",
    "    system_message=\"Creative in software product ideas.\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "groupchat = autogen.GroupChat(agents=[user_proxy, coder, pm], messages=[], max_round=20)\n",
    "\n",
    "# replace GroupChatManager with CompressibleGroupChatManager\n",
    "# manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=gpt4_config)\n",
    "from autogen.agentchat.contrib.compressible_groupchatmanager import CompressibleGroupChatManager\n",
    "manager = CompressibleGroupChatManager(\n",
    "    groupchat=groupchat, \n",
    "    llm_config=llm_config,\n",
    "    compress_config={\n",
    "        \"mode\": \"COMPRESS\",\n",
    "        \"trigger_count\": 2000,\n",
    "        \"leave_last_n\": 2, # leave last n messages uncompressed\n",
    "        \"verbose\": False, # to allow printing of compression information\n",
    "    })\n",
    "\n",
    "print(f\"Initial token count from manager: {manager.init_token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "To achieve the task, we need to perform two steps:\n",
      "\n",
      "1. We need to search the latest papers about GPT-4 on arXiv.\n",
      "2. Once we have the paper details, we need to extract information related to GPT-4's potential applications in software from it.\n",
      "\n",
      "I can help to get automated search results from arXiv using its API. However, fetching data from downloaded papers is more complex, as they are normally in PDF format. An AI model can't read and comprehend text from a PDF in the same way a human would, but I can assist you to get the paper, you would need to manually read it for detailed information.\n",
      "\n",
      "Let's start with the first step, which is to find the latest research paper about GPT-4 on arXiv. We will use the `arxiv` python library for this, which is a Python wrapper for the arXiv API. Follow along with this python code.\n",
      "\n",
      "Note: You may need to install the `arxiv` library if it's not already installed on your system. You can do that by running `pip install arxiv`.\n",
      "\n",
      "```python\n",
      "# import required library\n",
      "import arxiv\n",
      "\n",
      "# search for the latest papers on GPT-4\n",
      "search = arxiv.Search(\n",
      "  query = \"GPT-4\",\n",
      "  max_results = 1,\n",
      "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
      ")\n",
      "\n",
      "# print the latest paper details\n",
      "for result in search.get():\n",
      "  print(f\"Title: {result.title}\")\n",
      "  print(f\"Author(s): {', '.join(result.authors)}\")\n",
      "  print(f\"Abstract: {result.summary}\")\n",
      "  print(f\"Link: {result.pdf_url}\")\n",
      "```\n",
      "\n",
      "Once you get the paper details, you can manually check the paper for potential applications of GPT-4 in software.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "Traceback (most recent call last):\n",
      "  File \"\", line 12, in <module>\n",
      "    for result in search.get():\n",
      "AttributeError: 'Search' object has no attribute 'get'\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "I apologize for the confusion. The `Search` object indeed doesn't have a `get()` method. We should iterate over the `Search` object directly.\n",
      "\n",
      "Directly iterating over the `Search` object will give the desired result. This is because the `Search` object is an iterable and it uses `get()` method internally.\n",
      "\n",
      "Here is the corrected Python code:\n",
      "\n",
      "```python\n",
      "# import required library\n",
      "import arxiv\n",
      "\n",
      "# search for the latest papers on GPT-4\n",
      "search = arxiv.Search(\n",
      "  query = \"GPT-4\",\n",
      "  max_results = 1,\n",
      "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
      ")\n",
      "\n",
      "# print the latest paper details\n",
      "for result in search.results():\n",
      "  print(f\"Title: {result.title}\")\n",
      "  print(f\"Author(s): {', '.join(result.authors)}\")\n",
      "  print(f\"Abstract: {result.summary}\")\n",
      "  print(f\"Link: {result.pdf_url}\")\n",
      "```\n",
      "\n",
      "This will correctly print the details of the most recent paper on GPT-4 as per the arXiv API results.\n",
      "\n",
      "Please run the corrected code and let me know the output.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      ":12: DeprecationWarning: The '(Search).results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n",
      "Traceback (most recent call last):\n",
      "  File \"\", line 14, in <module>\n",
      "    print(f\"Author(s): {', '.join(result.authors)}\")\n",
      "TypeError: sequence item 0: expected str instance, Author found\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "I apologize for the confusion again. It seems there has been some changes in the way authors are represented in the latest versions of the arxiv library. The authors are now returned as `Author` objects.\n",
      "\n",
      "Let's modify the code to handle this:\n",
      "\n",
      "```python\n",
      "# import required library\n",
      "import arxiv\n",
      "\n",
      "# search for the latest papers on GPT-4\n",
      "search = arxiv.Search(\n",
      "  query = \"GPT-4\",\n",
      "  max_results = 1,\n",
      "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
      ")\n",
      "\n",
      "# print the latest paper details\n",
      "for result in search.get():\n",
      "  print(f\"Title: {result.title}\")\n",
      "\n",
      "  # Build author list as string\n",
      "  authors = \", \".join([str(author) for author in result.authors])\n",
      "  print(f\"Author(s): {authors}\")\n",
      "\n",
      "  print(f\"Abstract: {result.summary}\")\n",
      "  print(f\"Link: {result.pdf_url}\")\n",
      "```\n",
      "\n",
      "Please run the corrected code and let me know the output.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "Traceback (most recent call last):\n",
      "  File \"\", line 12, in <module>\n",
      "    for result in search.get():\n",
      "AttributeError: 'Search' object has no attribute 'get'\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "I apologize for the confusion earlier. The 'get()' method was inadvertently left in the code. In fact, we need to iterate over the `Search` object directly. This required change has been reflected in the code provided below.\n",
      "\n",
      "```python\n",
      "# import required library\n",
      "import arxiv\n",
      "\n",
      "# search for the latest papers on GPT-4\n",
      "search = arxiv.Search(\n",
      "  query = \"GPT-4\",\n",
      "  max_results = 1,\n",
      "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
      ")\n",
      "\n",
      "# print the latest paper details\n",
      "for result in search.results():\n",
      "  print(f\"Title: {result.title}\")\n",
      "\n",
      "  # Build author list as string\n",
      "  authors = \", \".join([str(author) for author in result.authors])\n",
      "  print(f\"Author(s): {authors}\")\n",
      "\n",
      "  print(f\"Abstract: {result.summary}\")\n",
      "  print(f\"Link: {result.pdf_url}\")\n",
      "```\n",
      "\n",
      "Please run this corrected code. My apologies for any confusion caused by the previous erroneous code suggestions. I'm here to help, so please don't hesitate to bring up any additional issues if they arise.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Title: Large Language Models as Automated Aligners for benchmarking Vision-Language Models\n",
      "Author(s): Yuanfeng Ji, Chongjian Ge, Weikai Kong, Enze Xie, Zhengying Liu, Zhengguo Li, Ping Luo\n",
      "Abstract: With the advancements in Large Language Models (LLMs), Vision-Language Models\n",
      "(VLMs) have reached a new level of sophistication, showing notable competence\n",
      "in executing intricate cognition and reasoning tasks. However, existing\n",
      "evaluation benchmarks, primarily relying on rigid, hand-crafted datasets to\n",
      "measure task-specific performance, face significant limitations in assessing\n",
      "the alignment of these increasingly anthropomorphic models with human\n",
      "intelligence. In this work, we address the limitations via Auto-Bench, which\n",
      "delves into exploring LLMs as proficient aligners, measuring the alignment\n",
      "between VLMs and human intelligence and value through automatic data curation\n",
      "and assessment. Specifically, for data curation, Auto-Bench utilizes LLMs\n",
      "(e.g., GPT-4) to automatically generate a vast set of question-answer-reasoning\n",
      "triplets via prompting on visual symbolic representations (e.g., captions,\n",
      "object locations, instance relationships, and etc.). The curated data closely\n",
      "matches human intent, owing to the extensive world knowledge embedded in LLMs.\n",
      "Through this pipeline, a total of 28.5K human-verified and 3,504K unfiltered\n",
      "question-answer-reasoning triplets have been curated, covering 4 primary\n",
      "abilities and 16 sub-abilities. We subsequently engage LLMs like GPT-3.5 to\n",
      "serve as judges, implementing the quantitative and qualitative automated\n",
      "assessments to facilitate a comprehensive evaluation of VLMs. Our validation\n",
      "results reveal that LLMs are proficient in both evaluation data curation and\n",
      "model assessment, achieving an average agreement rate of 85%. We envision\n",
      "Auto-Bench as a flexible, scalable, and comprehensive benchmark for evaluating\n",
      "the evolving sophisticated VLMs.\n",
      "Link: http://arxiv.org/pdf/2311.14580v1\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35mToken Count (including 567 tokens from system msg and function descriptions). Before compression : 2316 | After: 1691\u001b[0m\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mProduct_manager\u001b[0m (to chat_manager):\n",
      "\n",
      "Great, the code was executed successfully and we are able to get the latest paper about GPT-4 from arXiv. In terms of potential applications in software development, having scanned through the abstract, the main features lie in:\n",
      "\n",
      "1. **Automated Data Curation with LLMs**: Large language models like GPT-4 can help in automating data curation process. The generated question-answer-reasoning triplets can be utilized to train other models, enabling the development of more sophisticated AI tools, which can potentially enhance multiple spheres of the software development process, such as generating reliable and meaningful test data, debugging, documentation etc.\n",
      "\n",
      "2. **Assessment and Validation of Vision-Language Models(VLMs)**: VLMs are an integral part of software that deal with complex visual and language data. Fine-tuned VLMs can significantly improve software's capability in interpreting visual data and generating natural language output. A range of applications from image recognition and text generation to chatbots and virtual assistants could benefit from it.\n",
      "\n",
      "3. **Tool for Benchmarking AI Models**: The notion of using an LLM like GPT-4 to benchmark other AI models is a compelling application. This work could be used to develop new types of benchmarking software that provide more nuanced insights into model performance. Software that effectively measures the performance of AI models is quite important for developers. \n",
      "\n",
      "4. **Automated Test Generation**: This application of GPT-4 could be utilized in software testing, where GPT-4 can be used to generate test scenarios and cases to ensure comprehensive testing. \n",
      "\n",
      "Remember, this is a basic analysis. For a more comprehensive understanding of its applications, reading the full paper would be necessary.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35mToken Count (including 567 tokens from system msg and function descriptions). Before compression : 2042 | After: 1702\u001b[0m\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "user_proxy.initiate_chat(manager, message=\"Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\")\n",
    "# type exit to terminate the chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flaml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
