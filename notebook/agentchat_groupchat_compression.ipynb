{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_groupchat_compression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Generated Agent Chat: Group Chat with CompressibleGroupChatManager (Experimental)\n",
    "\n",
    "AutoGen offers conversable agents powered by LLM, tool or human, which can be used to perform tasks collectively via automated chat. This framework allows tool use and human participation through multi-agent conversation.\n",
    "Please find documentation about this feature [here](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat).\n",
    "\n",
    "In this notebook, we demonstrate how to use an experimental version of AutoGen's GroupChatManager, `CompressibleGroupChatManager`, to compress the conversation history.\n",
    "- If `compress_config=False`, the `CompressibleGroupChatManager` is equivalent to `GroupChatManager`. \n",
    "- If turned on, it will perform compression when a pre-set token count is reached. The compressed messages will be broadcasted to all agents in the conversation.\n",
    "\n",
    "There are different modes to be used: `TERMINATE, COMPRESS, CUMSTOMIZED`. You can read more about compression and compressible agents in `agentchat_compression.ipynb`. \n",
    "\n",
    "## Requirements\n",
    "\n",
    "AutoGen requires `Python>=3.8`. To run this notebook example, please install:\n",
    "```bash\n",
    "pip install pyautogen\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "# %pip install pyautogen~=0.1.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "The [`config_list_from_json`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "config_list_gpt4 = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4\", \"gpt-4-0314\", \"gpt4\", \"gpt-4-32k\", \"gpt-4-32k-0314\", \"gpt-4-32k-v0314\"],\n",
    "    },\n",
    ")\n",
    "# config_list_gpt35 = autogen.config_list_from_json(\n",
    "#     \"OAI_CONFIG_LIST\",\n",
    "#     filter_dict={\n",
    "#         \"model\": {\n",
    "#             \"gpt-3.5-turbo\",\n",
    "#             \"gpt-3.5-turbo-16k\",\n",
    "#             \"gpt-3.5-turbo-0301\",\n",
    "#             \"chatgpt-35-turbo-0301\",\n",
    "#             \"gpt-35-turbo-v0301\",\n",
    "#         },\n",
    "#     },\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It first looks for environment variable \"OAI_CONFIG_LIST\" which needs to be a valid json string. If that variable is not found, it then looks for a json file named \"OAI_CONFIG_LIST\". It filters the configs by models (you can filter by other keys as well). Only the gpt-4 models are kept in the list based on the filter condition.\n",
    "\n",
    "The config list looks like the following:\n",
    "```python\n",
    "config_list = [\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your OpenAI API key here>',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'base_url': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-4-32k',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'base_url': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "]\n",
    "```\n",
    "\n",
    "If you open this notebook in colab, you can upload your files by clicking the file icon on the left panel and then choose \"upload file\" icon.\n",
    "\n",
    "You can set the value of config_list in other ways you prefer, e.g., loading from a YAML file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example group chat with compression\n",
    "\n",
    "This example is from [agentchat_groupchat.ipynb](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_groupchat.ipynb). \n",
    "\n",
    "Simply replace `GroupChatManager` with `CompressibleGroupChatManager` to enable compression. Refer to `agentchat_compression.ipynb` for other ways of handling long conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "config_list = [\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': open('key_openai.txt', 'r').read().strip(),\n",
    "\n",
    "    },\n",
    "]\n",
    "llm_config = {\"config_list\": config_list, \"seed\": 42}\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "   name=\"User_proxy\",\n",
    "   system_message=\"A human admin.\",\n",
    "   code_execution_config={\"last_n_messages\": 2, \"work_dir\": \"groupchat\"},\n",
    "   human_input_mode=\"TERMINATE\"\n",
    ")\n",
    "coder = autogen.AssistantAgent(\n",
    "    name=\"Coder\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "pm = autogen.AssistantAgent(\n",
    "    name=\"Product_manager\",\n",
    "    system_message=\"Creative in software product ideas.\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "groupchat = autogen.GroupChat(agents=[user_proxy, coder, pm], messages=[], max_round=10)\n",
    "\n",
    "# replace GroupChatManager with CompressibleGroupChatManager\n",
    "# manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=gpt4_config)\n",
    "from autogen.agentchat.contrib.compressible_groupchat import CompressibleGroupChatManager\n",
    "manager = CompressibleGroupChatManager(\n",
    "    groupchat=groupchat, \n",
    "    llm_config=llm_config,\n",
    "    compress_config={\n",
    "        \"mode\": \"COMPRESS\",\n",
    "        \"trigger_count\": 2000,\n",
    "        \"leave_last_n\": 2, # number of messages to consider for compression\n",
    "        \"verbose\": True, # to allow printing of compression information\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "To achieve the task, we need to perform two steps:\n",
      "\n",
      "1. We need to search the latest papers about GPT-4 on arXiv.\n",
      "2. Once we have the paper details, we need to extract information related to GPT-4's potential applications in software from it.\n",
      "\n",
      "I can help to get automated search results from arXiv using its API. However, fetching data from downloaded papers is more complex, as they are normally in PDF format. An AI model can't read and comprehend text from a PDF in the same way a human would, but I can assist you to get the paper, you would need to manually read it for detailed information.\n",
      "\n",
      "Let's start with the first step, which is to find the latest research paper about GPT-4 on arXiv. We will use the `arxiv` python library for this, which is a Python wrapper for the arXiv API. Follow along with this python code.\n",
      "\n",
      "Note: You may need to install the `arxiv` library if it's not already installed on your system. You can do that by running `pip install arxiv`.\n",
      "\n",
      "```python\n",
      "# import required library\n",
      "import arxiv\n",
      "\n",
      "# search for the latest papers on GPT-4\n",
      "search = arxiv.Search(\n",
      "  query = \"GPT-4\",\n",
      "  max_results = 1,\n",
      "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
      ")\n",
      "\n",
      "# print the latest paper details\n",
      "for result in search.get():\n",
      "  print(f\"Title: {result.title}\")\n",
      "  print(f\"Author(s): {', '.join(result.authors)}\")\n",
      "  print(f\"Abstract: {result.summary}\")\n",
      "  print(f\"Link: {result.pdf_url}\")\n",
      "```\n",
      "\n",
      "Once you get the paper details, you can manually check the paper for potential applications of GPT-4 in software.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "Traceback (most recent call last):\n",
      "  File \"\", line 12, in <module>\n",
      "    for result in search.get():\n",
      "AttributeError: 'Search' object has no attribute 'get'\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "I apologize for the confusion. The `Search` object indeed doesn't have a `get()` method. We should iterate over the `Search` object directly.\n",
      "\n",
      "Directly iterating over the `Search` object will give the desired result. This is because the `Search` object is an iterable and it uses `get()` method internally.\n",
      "\n",
      "Here is the corrected Python code:\n",
      "\n",
      "```python\n",
      "# import required library\n",
      "import arxiv\n",
      "\n",
      "# search for the latest papers on GPT-4\n",
      "search = arxiv.Search(\n",
      "  query = \"GPT-4\",\n",
      "  max_results = 1,\n",
      "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
      ")\n",
      "\n",
      "# print the latest paper details\n",
      "for result in search.results():\n",
      "  print(f\"Title: {result.title}\")\n",
      "  print(f\"Author(s): {', '.join(result.authors)}\")\n",
      "  print(f\"Abstract: {result.summary}\")\n",
      "  print(f\"Link: {result.pdf_url}\")\n",
      "```\n",
      "\n",
      "This will correctly print the details of the most recent paper on GPT-4 as per the arXiv API results.\n",
      "\n",
      "Please run the corrected code and let me know the output.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      ":12: DeprecationWarning: The '(Search).results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n",
      "Traceback (most recent call last):\n",
      "  File \"\", line 14, in <module>\n",
      "    print(f\"Author(s): {', '.join(result.authors)}\")\n",
      "TypeError: sequence item 0: expected str instance, Author found\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "I apologize for the confusion again. It seems there has been some changes in the way authors are represented in the latest versions of the arxiv library. The authors are now returned as `Author` objects.\n",
      "\n",
      "Let's modify the code to handle this:\n",
      "\n",
      "```python\n",
      "# import required library\n",
      "import arxiv\n",
      "\n",
      "# search for the latest papers on GPT-4\n",
      "search = arxiv.Search(\n",
      "  query = \"GPT-4\",\n",
      "  max_results = 1,\n",
      "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
      ")\n",
      "\n",
      "# print the latest paper details\n",
      "for result in search.get():\n",
      "  print(f\"Title: {result.title}\")\n",
      "\n",
      "  # Build author list as string\n",
      "  authors = \", \".join([str(author) for author in result.authors])\n",
      "  print(f\"Author(s): {authors}\")\n",
      "\n",
      "  print(f\"Abstract: {result.summary}\")\n",
      "  print(f\"Link: {result.pdf_url}\")\n",
      "```\n",
      "\n",
      "Please run the corrected code and let me know the output.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "Traceback (most recent call last):\n",
      "  File \"\", line 12, in <module>\n",
      "    for result in search.get():\n",
      "AttributeError: 'Search' object has no attribute 'get'\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "I apologize for the confusion earlier. The 'get()' method was inadvertently left in the code. In fact, we need to iterate over the `Search` object directly. This required change has been reflected in the code provided below.\n",
      "\n",
      "```python\n",
      "# import required library\n",
      "import arxiv\n",
      "\n",
      "# search for the latest papers on GPT-4\n",
      "search = arxiv.Search(\n",
      "  query = \"GPT-4\",\n",
      "  max_results = 1,\n",
      "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
      ")\n",
      "\n",
      "# print the latest paper details\n",
      "for result in search.results():\n",
      "  print(f\"Title: {result.title}\")\n",
      "\n",
      "  # Build author list as string\n",
      "  authors = \", \".join([str(author) for author in result.authors])\n",
      "  print(f\"Author(s): {authors}\")\n",
      "\n",
      "  print(f\"Abstract: {result.summary}\")\n",
      "  print(f\"Link: {result.pdf_url}\")\n",
      "```\n",
      "\n",
      "Please run this corrected code. My apologies for any confusion caused by the previous erroneous code suggestions. I'm here to help, so please don't hesitate to bring up any additional issues if they arise.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Title: Generative Input: Towards Next-Generation Input Methods Paradigm\n",
      "Author(s): Keyu Ding, Yongcan Wang, Zihang Xu, Zhenzhen Jia, Shijin Wang, Cong Liu, Enhong Chen\n",
      "Abstract: Since the release of ChatGPT, generative models have achieved tremendous\n",
      "success and become the de facto approach for various NLP tasks. However, its\n",
      "application in the field of input methods remains under-explored. Many neural\n",
      "network approaches have been applied to the construction of Chinese input\n",
      "method engines(IMEs).Previous research often assumed that the input pinyin was\n",
      "correct and focused on Pinyin-to-character(P2C) task, which significantly falls\n",
      "short of meeting users' demands. Moreover, previous research could not leverage\n",
      "user feedback to optimize the model and provide personalized results. In this\n",
      "study, we propose a novel Generative Input paradigm named GeneInput. It uses\n",
      "prompts to handle all input scenarios and other intelligent auxiliary input\n",
      "functions, optimizing the model with user feedback to deliver personalized\n",
      "results. The results demonstrate that we have achieved state-of-the-art\n",
      "performance for the first time in the Full-mode Key-sequence to\n",
      "Characters(FK2C) task. We propose a novel reward model training method that\n",
      "eliminates the need for additional manual annotations and the performance\n",
      "surpasses GPT-4 in tasks involving intelligent association and conversational\n",
      "assistance. Compared to traditional paradigms, GeneInput not only demonstrates\n",
      "superior performance but also exhibits enhanced robustness, scalability, and\n",
      "online learning capabilities.\n",
      "Link: http://arxiv.org/pdf/2311.01166v1\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m******************************Start compressing the following content:******************************\u001b[0m\n",
      "To be compressed:\n",
      "##Coder(USER)## To achieve the task, we need to perform two steps:\n",
      "\n",
      "1. We need to search the latest papers about GPT-4 on arXiv.\n",
      "2. Once we have the paper details, we need to extract information related to GPT-4's potential applications in software from it.\n",
      "\n",
      "I can help to get automated search results from arXiv using its API. However, fetching data from downloaded papers is more complex, as they are normally in PDF format. An AI model can't read and comprehend text from a PDF in the same way a human would, but I can assist you to get the paper, you would need to manually read it for detailed information.\n",
      "\n",
      "Let's start with the first step, which is to find the latest research paper about GPT-4 on arXiv. We will use the `arxiv` python library for this, which is a Python wrapper for the arXiv API. Follow along with this python code.\n",
      "\n",
      "Note: You may need to install the `arxiv` library if it's not already installed on your system. You can do that by running `pip install arxiv`.\n",
      "\n",
      "```python\n",
      "# import required library\n",
      "import arxiv\n",
      "\n",
      "# search for the latest papers on GPT-4\n",
      "search = arxiv.Search(\n",
      "  query = \"GPT-4\",\n",
      "  max_results = 1,\n",
      "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
      ")\n",
      "\n",
      "# print the latest paper details\n",
      "for result in search.get():\n",
      "  print(f\"Title: {result.title}\")\n",
      "  print(f\"Author(s): {', '.join(result.authors)}\")\n",
      "  print(f\"Abstract: {result.summary}\")\n",
      "  print(f\"Link: {result.pdf_url}\")\n",
      "```\n",
      "\n",
      "Once you get the paper details, you can manually check the paper for potential applications of GPT-4 in software.\n",
      "##User_proxy(USER)## exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "Traceback (most recent call last):\n",
      "  File \"\", line 12, in <module>\n",
      "    for result in search.get():\n",
      "AttributeError: 'Search' object has no attribute 'get'\n",
      "\n",
      "##Coder(USER)## I apologize for the confusion. The `Search` object indeed doesn't have a `get()` method. We should iterate over the `Search` object directly.\n",
      "\n",
      "Directly iterating over the `Search` object will give the desired result. This is because the `Search` object is an iterable and it uses `get()` method internally.\n",
      "\n",
      "Here is the corrected Python code:\n",
      "\n",
      "```python\n",
      "# import required library\n",
      "import arxiv\n",
      "\n",
      "# search for the latest papers on GPT-4\n",
      "search = arxiv.Search(\n",
      "  query = \"GPT-4\",\n",
      "  max_results = 1,\n",
      "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
      ")\n",
      "\n",
      "# print the latest paper details\n",
      "for result in search.results():\n",
      "  print(f\"Title: {result.title}\")\n",
      "  print(f\"Author(s): {', '.join(result.authors)}\")\n",
      "  print(f\"Abstract: {result.summary}\")\n",
      "  print(f\"Link: {result.pdf_url}\")\n",
      "```\n",
      "\n",
      "This will correctly print the details of the most recent paper on GPT-4 as per the arXiv API results.\n",
      "\n",
      "Please run the corrected code and let me know the output.\n",
      "##User_proxy(USER)## exitcode: 1 (execution failed)\n",
      "Code output: \n",
      ":12: DeprecationWarning: The '(Search).results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n",
      "Traceback (most recent call last):\n",
      "  File \"\", line 14, in <module>\n",
      "    print(f\"Author(s): {', '.join(result.authors)}\")\n",
      "TypeError: sequence item 0: expected str instance, Author found\n",
      "\n",
      "##Coder(USER)## I apologize for the confusion again. It seems there has been some changes in the way authors are represented in the latest versions of the arxiv library. The authors are now returned as `Author` objects.\n",
      "\n",
      "Let's modify the code to handle this:\n",
      "\n",
      "```python\n",
      "# import required library\n",
      "import arxiv\n",
      "\n",
      "# search for the latest papers on GPT-4\n",
      "search = arxiv.Search(\n",
      "  query = \"GPT-4\",\n",
      "  max_results = 1,\n",
      "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
      ")\n",
      "\n",
      "# print the latest paper details\n",
      "for result in search.get():\n",
      "  print(f\"Title: {result.title}\")\n",
      "\n",
      "  # Build author list as string\n",
      "  authors = \", \".join([str(author) for author in result.authors])\n",
      "  print(f\"Author(s): {authors}\")\n",
      "\n",
      "  print(f\"Abstract: {result.summary}\")\n",
      "  print(f\"Link: {result.pdf_url}\")\n",
      "```\n",
      "\n",
      "Please run the corrected code and let me know the output.\n",
      "##User_proxy(USER)## exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "Traceback (most recent call last):\n",
      "  File \"\", line 12, in <module>\n",
      "    for result in search.get():\n",
      "AttributeError: 'Search' object has no attribute 'get'\n",
      "\n",
      "\n",
      "\u001b[35m******************************Content after compressing:******************************\u001b[0m\n",
      "##Coder(USER)##: Outlines a two-step task - finding latest GPT-4 papers on arXiv and extracting GPT-4 applications in software from these papers. Provides a Python code, using 'arxiv' library, for the first step. Noted that paper data extraction is complex due to PDF format and requires manual review.\n",
      "##User_proxy(USER)##: Reports a code execution failure due to an attribute error in the 'Search' object.\n",
      "##Coder(USER)##: Corrects the error by iterating over the 'Search' object directly and shares revised code.\n",
      "##User_proxy(USER)##: Executes the revised code but fails again due to a deprecation warning and a TypeError.\n",
      "##Coder(USER)##: Responds by editing the code to handle 'Author' objects, instructs the user to run it.\n",
      "##User_proxy(USER)##: Executes the updated code but fails again due to the same attribute error in 'Search' object. \u001b[35m\n",
      "********************************************************************************\u001b[0m\n",
      "1669 843\n",
      "\u001b[35mToken Count (including 530 tokens from system msg and function descriptions). Before compression : 2233 | After: 1373\u001b[0m\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "We found the most recent paper about GPT-4 on arxiv. The title of the paper is: \"Generative Input: Towards Next-Generation Input Methods Paradigm\", by authors Keyu Ding, Yongcan Wang, Zihang Xu, Zhenzhen Jia, Shijin Wang, Cong Liu, and Enhong Chen. Please find the abstract below:\n",
      "\n",
      "\"Since the release of ChatGPT, generative models have achieved tremendous success and become the de facto approach for various NLP tasks. However, its application in the field of input methods remains under-explored. Many neural network approaches have been applied to the construction of Chinese input method engines(IMEs). Previous research often assumed that the input pinyin was correct and focused on Pinyin-to-character(P2C) task, which significantly falls short of meeting users' demands. Moreover, previous research could not leverage user feedback to optimize the model and provide personalized results. In this study, we propose a novel Generative Input paradigm named GeneInput. It uses prompts to handle all input scenarios and other intelligent auxiliary input functions, optimizing the model with user feedback to deliver personalized results. The results demonstrate that we have achieved state-of-the-art performance for the first time in the Full-mode Key-sequence to Characters (FK2C) task. We propose a novel reward model training method that eliminates the need for additional manual annotations and the performance surpasses GPT-4 in tasks involving intelligent association and conversational assistance. Compared to traditional paradigms, GeneInput not only demonstrates superior performance but also exhibits enhanced robustness, scalability, and online learning capabilities.\"\n",
      "\n",
      "Unfortunately, the paper does not strictly focus on GPT-4, it rather discusses a new generative model named GeneInput for an application in the field of input methods that were previously less explored. The potential applications in software from this utilization could include more intelligent, user-friendly, and personalized input methods. The paper also implies that such generative input might be useful for applications such as autocorrection, language translation, and conversational assistance tasks.\n",
      "\n",
      "This is, however, just one example. There might be other papers touching the potential applications of GPT-4 in software, but a more detailed inspection is needed. For that manual review of the content is required, as the AI models are currently incapable of complex content comprehension and analysis.\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "user_proxy.initiate_chat(manager, message=\"Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\")\n",
    "# type exit to terminate the chat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flaml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
