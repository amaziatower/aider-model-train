{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "tags: [\"RAG\"]\n",
    "description: |\n",
    "    Explore the use of AutoGen's RagAgent for tasks like code generation from docstrings, answering complex questions with human feedback, and exploiting features like Update Context, custom prompts, and few-shot learning.\n",
    "-->\n",
    "\n",
    "# Using RagAgent for Retrieval-Augmented Code Generation and Question Answering\n",
    "\n",
    "Introducing an agent capable of performing Retrieval-Augmented Generation (RAG) for the given message.\n",
    "\n",
    "Upon receipt of a message, the agent employs RAG to generate a reply. It retrieves documents based on the message, then generates a reply using both the retrieved documents and the message itself. Additionally, it supports automatic context updates during the conversation, either autonomously or at the user`s request.\n",
    "\n",
    "We also support enabling the RAG capability for any conversable agent with `Ragability`.\n",
    "\n",
    "## Table of Contents\n",
    "We'll demonstrate six examples of using RetrieveChat for code generation and question answering:\n",
    "\n",
    "- [Example 1: Use RagAgent to help generate code](#example-1)\n",
    "- [Example 2: Use RagAgent to answer a multi-hop question.](#example-2)\n",
    "- [Example 3: Enable RAG capability for any conversable agent.](#example-3)\n",
    "\n",
    "````{=mdx}\n",
    ":::info Requirements\n",
    "Some extra dependencies are needed for this notebook, which can be installed via pip:\n",
    "\n",
    "```bash\n",
    "pip install pyautogen[rag] flaml[automl]\n",
    "```\n",
    "\n",
    "For more information, please refer to the [installation guide](/docs/installation/).\n",
    ":::\n",
    "````"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "The [`config_list_from_json`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lijiang1/anaconda3/envs/autogen/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models to use:  ['gpt-35-turbo', 'gpt-35-turbo-0613']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import autogen\n",
    "from autogen.agentchat.contrib.rag import RagAgent, logger\n",
    "from autogen.agentchat.contrib.rag.splitter import SUPPORTED_EXTENSIONS\n",
    "from autogen.agentchat.contrib.capabilities.rag_capability import Ragability\n",
    "\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "config_list = autogen.config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\")\n",
    "\n",
    "assert len(config_list) > 0\n",
    "print(\"models to use: \", [config_list[i][\"model\"] for i in range(len(config_list))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "configuration).\n",
    ":::\n",
    "````\n",
    "\n",
    "## Construct agents for RAG\n",
    "\n",
    "We start by initializing the `RagAgent` and `UserProxyAgent`. The `RagAgent` will be responsible for replying to `UserProxyAgent` with retrieval-augmented generation based on the given knowledge base contained in the `docs_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepted file formats for `docs_path`: ['.txt', '.json', '.csv', '.tsv', '.md', '.html', '.htm', '.rtf', '.rst', '.jsonl', '.log', '.xml', '.yaml', '.yml', '.pdf']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accepted file formats for `docs_path`: {SUPPORTED_EXTENSIONS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lijiang1/anaconda3/envs/autogen/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "2024-02-26 08:04:12,971 - utils.py:   16 - DEBUG - Processing file: ./tmp/download/Integrate%20-%20Spark.md, url: https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Examples/Integrate%20-%20Spark.md\u001b[0m\n",
      "2024-02-26 08:04:13,334 - utils.py:   16 - DEBUG - \u001b[32mSplit ./tmp/download/Integrate%20-%20Spark.md into 2 chunks.\u001b[0m\n",
      "2024-02-26 08:04:13,335 - utils.py:   16 - DEBUG - Processing file: ./tmp/download/Research.md, url: https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Research.md\u001b[0m\n",
      "2024-02-26 08:04:13,338 - utils.py:   16 - DEBUG - \u001b[32mSplit ./tmp/download/Research.md into 2 chunks.\u001b[0m\n",
      "2024-02-26 08:04:13,338 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/Research.md, url: None\u001b[0m\n",
      "2024-02-26 08:04:13,340 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/Research.md into 1 chunks.\u001b[0m\n",
      "2024-02-26 08:04:13,340 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/Contribute.md, url: None\u001b[0m\n",
      "2024-02-26 08:04:13,345 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/Contribute.md into 3 chunks.\u001b[0m\n",
      "2024-02-26 08:04:13,346 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/Ecosystem.md, url: None\u001b[0m\n",
      "2024-02-26 08:04:13,347 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/Ecosystem.md into 1 chunks.\u001b[0m\n",
      "2024-02-26 08:04:13,347 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/Examples.md, url: None\u001b[0m\n",
      "2024-02-26 08:04:13,351 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/Examples.md into 3 chunks.\u001b[0m\n",
      "2024-02-26 08:04:13,352 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/FAQ.md, url: None\u001b[0m\n",
      "2024-02-26 08:04:13,357 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/FAQ.md into 4 chunks.\u001b[0m\n",
      "2024-02-26 08:04:13,358 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/Getting-Started.md, url: None\u001b[0m\n",
      "2024-02-26 08:04:13,360 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/Getting-Started.md into 2 chunks.\u001b[0m\n",
      "2024-02-26 08:04:13,361 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/Migration-Guide.md, url: None\u001b[0m\n",
      "2024-02-26 08:04:13,362 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/Migration-Guide.md into 1 chunks.\u001b[0m\n",
      "2024-02-26 08:04:13,362 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/Use-Cases/agent_chat.md, url: None\u001b[0m\n",
      "2024-02-26 08:04:13,370 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/Use-Cases/agent_chat.md into 6 chunks.\u001b[0m\n",
      "2024-02-26 08:04:13,370 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/Use-Cases/enhanced_inference.md, url: None\u001b[0m\n",
      "2024-02-26 08:04:13,380 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/Use-Cases/enhanced_inference.md into 6 chunks.\u001b[0m\n",
      "2024-02-26 08:04:13,380 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/installation/Docker.md, url: None\u001b[0m\n",
      "2024-02-26 08:04:13,383 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/installation/Docker.md into 2 chunks.\u001b[0m\n",
      "2024-02-26 08:04:13,383 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/installation/Optional-Dependencies.md, url: None\u001b[0m\n",
      "2024-02-26 08:04:13,385 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/installation/Optional-Dependencies.md into 1 chunks.\u001b[0m\n",
      "2024-02-26 08:04:13,386 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/Research.md, url: None\u001b[0m\n",
      "2024-02-26 08:04:13,387 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/Research.md into 1 chunks.\u001b[0m\n",
      "2024-02-26 08:04:13,387 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/Contribute.md, url: None\u001b[0m\n",
      "2024-02-26 08:04:13,392 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/Contribute.md into 3 chunks.\u001b[0m\n",
      "2024-02-26 08:04:13,393 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/Ecosystem.md, url: None\u001b[0m\n",
      "2024-02-26 08:04:13,394 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/Ecosystem.md into 1 chunks.\u001b[0m\n",
      "2024-02-26 08:04:13,394 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/Examples.md, url: None\u001b[0m\n",
      "2024-02-26 08:04:13,398 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/Examples.md into 3 chunks.\u001b[0m\n",
      "2024-02-26 08:04:13,398 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/FAQ.md, url: None\u001b[0m\n",
      "2024-02-26 08:04:13,404 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/FAQ.md into 4 chunks.\u001b[0m\n",
      "2024-02-26 08:04:13,404 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/Getting-Started.md, url: None\u001b[0m\n",
      "2024-02-26 08:04:13,406 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/Getting-Started.md into 2 chunks.\u001b[0m\n",
      "2024-02-26 08:04:13,407 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/Migration-Guide.md, url: None\u001b[0m\n",
      "2024-02-26 08:04:13,408 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/Migration-Guide.md into 1 chunks.\u001b[0m\n",
      "2024-02-26 08:04:13,409 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/Use-Cases/agent_chat.md, url: None\u001b[0m\n",
      "2024-02-26 08:04:13,416 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/Use-Cases/agent_chat.md into 6 chunks.\u001b[0m\n",
      "2024-02-26 08:04:13,417 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/Use-Cases/enhanced_inference.md, url: None\u001b[0m\n",
      "2024-02-26 08:04:13,427 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/Use-Cases/enhanced_inference.md into 6 chunks.\u001b[0m\n",
      "2024-02-26 08:04:13,428 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/installation/Docker.md, url: None\u001b[0m\n",
      "2024-02-26 08:04:13,430 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/installation/Docker.md into 2 chunks.\u001b[0m\n",
      "2024-02-26 08:04:13,430 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/installation/Optional-Dependencies.md, url: None\u001b[0m\n",
      "2024-02-26 08:04:13,433 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/installation/Optional-Dependencies.md into 1 chunks.\u001b[0m\n",
      "2024-02-26 08:04:13,434 - utils.py:   16 - DEBUG - split took 0.52 seconds.\u001b[0m\n",
      "2024-02-26 08:04:15,612 - utils.py:   16 - DEBUG - encode_chunks took 2.18 seconds.\u001b[0m\n",
      "2024-02-26 08:04:15,794 - utils.py:   16 - DEBUG - insert_docs took 0.09 seconds.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "llm_config = {\n",
    "    \"timeout\": 60,\n",
    "    \"config_list\": config_list,\n",
    "}\n",
    "\n",
    "\n",
    "def termination_msg(x):\n",
    "    return isinstance(x, dict) and \"TERMINATE\" == str(x.get(\"content\", \"\"))[-9:].upper()\n",
    "\n",
    "\n",
    "userproxy = autogen.UserProxyAgent(\n",
    "    name=\"userproxy\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config=False,  # {\"use_docker\": False, \"work_dir\": \".tmp\"},\n",
    "    # default_auto_reply=\"Reply `TERMINATE` if the task is done.\",\n",
    "    description=\"The boss who ask questions and give tasks.\",\n",
    ")\n",
    "\n",
    "# Check the docstring of RagAgent for more details on the configuration.\n",
    "rag_config = {\n",
    "    \"docs_path\": [\n",
    "        \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Examples/Integrate%20-%20Spark.md\",\n",
    "        \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Research.md\",\n",
    "        os.path.join(os.path.abspath(\"\"), \"..\", \"website\", \"docs\"),\n",
    "    ],\n",
    "    \"overwrite\": True,  # set to False to avoid overwriting the existing documents\n",
    "}\n",
    "\n",
    "rag = RagAgent(\n",
    "    name=\"rag\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=5,\n",
    "    llm_config=llm_config,\n",
    "    rag_config=rag_config,\n",
    "    code_execution_config=False,\n",
    "    description=\"Assistant that can answer questions and generate code based on retrieved documents.\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "\n",
    "[Back to top](#table-of-contents)\n",
    "\n",
    "Use RagAgent to help generate sample code and automatically run the code and fix errors if there is any.\n",
    "\n",
    "Problem: How to use FLAML for a classification task and train the model in 30 seconds. Use spark to parallel the training. Force cancel jobs if time limit is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muserproxy\u001b[0m (to rag):\n",
      "\n",
      "How to use FLAML for a classification task and train the model in 30 seconds. Use spark to parallel the training. Force cancel jobs if time limit is reached.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 08:04:15,829 - utils.py:   16 - DEBUG - is_code_execution_result: False\u001b[0m\n",
      "2024-02-26 08:04:15,829 - utils.py:   16 - DEBUG - \u001b[32mInput message: How to use FLAML for a classification task and train the model in 30 seconds. Use spark to parallel the training. Force cancel jobs if time limit is reached.\u001b[0m\n",
      "2024-02-26 08:04:15,830 - utils.py:   16 - DEBUG - \u001b[32mReceived raw message: How to use FLAML for a classification task and train the model in 30 seconds. Use spark to parallel the training. Force cancel jobs if time limit is reached.\u001b[0m\n",
      "2024-02-26 08:04:15,830 - utils.py:   16 - DEBUG - \u001b[32mPerforming RAG for message: How to use FLAML for a classification task and train the model in 30 seconds. Use spark to parallel the training. Force cancel jobs if time limit is reached.\u001b[0m\n",
      "2024-02-26 08:04:15,859 - utils.py:   16 - DEBUG - \u001b[32mTask predicted: `code`\u001b[0m\n",
      "2024-02-26 08:04:15,860 - utils.py:   16 - DEBUG - __call__ took 0.03 seconds.\u001b[0m\n",
      "2024-02-26 08:04:15,860 - utils.py:   16 - DEBUG - \u001b[32mRefined message for db query: ['How to use FLAML for a classification task and train the model in 30 seconds', 'How to parallelize FLAML training using Spark and cancel jobs if the time limit is reached']\u001b[0m\n",
      "2024-02-26 08:04:16,094 - utils.py:   16 - DEBUG - retrieve_docs took 0.23 seconds.\u001b[0m\n",
      "2024-02-26 08:04:16,104 - utils.py:   16 - DEBUG - get_docs_by_ids took 0.01 seconds.\u001b[0m\n",
      "2024-02-26 08:04:16,108 - utils.py:   16 - DEBUG - rerank took 0.00 seconds.\u001b[0m\n",
      "2024-02-26 08:04:16,110 - utils.py:   16 - DEBUG - get_docs_by_ids took 0.00 seconds.\u001b[0m\n",
      "2024-02-26 08:04:16,111 - utils.py:   16 - DEBUG - convert_get_results_to_query_results took 0.00 seconds.\u001b[0m\n",
      "2024-02-26 08:04:16,111 - utils.py:   16 - DEBUG - perform_rag took 0.28 seconds.\u001b[0m\n",
      "2024-02-26 08:04:16,112 - utils.py:   16 - DEBUG - \u001b[32mUsed doc ids: set()\u001b[0m\n",
      "2024-02-26 08:04:16,114 - utils.py:   16 - DEBUG - \u001b[32mAdding doc_id 5162704023747046347 to context.\u001b[0m\n",
      "2024-02-26 08:04:16,116 - utils.py:   16 - DEBUG - \u001b[32mAdding doc_id 6841884977826051494 to context.\u001b[0m\n",
      "2024-02-26 08:04:16,116 - utils.py:   16 - DEBUG - process_message took 0.29 seconds.\u001b[0m\n",
      "2024-02-26 08:04:16,117 - utils.py:   16 - DEBUG - \u001b[32mTo LLM message: You're a retrieval augmented coding assistant. You answer user's questions based on your own knowled\u001b[0m\n",
      "2024-02-26 08:04:16,117 - utils.py:   16 - DEBUG - \u001b[32mTokens in history: 0\u001b[0m\n",
      "2024-02-26 08:04:16,139 - utils.py:   16 - DEBUG - Inner LLM reply: {'content': 'You can use FLAML for a classification task and train the model in 30 seconds while using Spark to parallelize the training. Force cancel jobs if the time limit is reached using the following code snippet:\\n\\n```python\\nimport pandas as pd\\nfrom pyspark.sql import SparkSession\\nfrom flaml import AutoML\\n\\nspark = SparkSession.builder \\\\\\n    .appName(\"FLAML\") \\\\\\n    .config(\"spark.sql.execution.arrow.enabled\",\"true\") \\\\\\n    .getOrCreate()\\n\\ntrain_data = spark.read.format(\"csv\").option(\"header\", \"true\").option(\\n    \"inferSchema\", \"true\").load(\"train.csv\")\\n\\ntrain_pd = train_data.toPandas()\\nX_train = train_pd.drop(columns=[\\'target\\'])\\ny_train = train_pd[\\'target\\']\\n\\nautoml = AutoML()\\n\\nsettings = {\\n    \"time_budget\": 30,\\n    \"metric\": \\'accuracy\\',\\n    \"task\": \\'classification\\',\\n    \"n_jobs\": -1,\\n    \"model_history\": True,\\n    \"log_file_name\": \"log.txt\",\\n    \"estimator_list\": [\\'lgbm\\', \\'rf\\', \\'xgboost\\'],\\n    \"use_spark\": True,\\n    \"force_cancel\": True,\\n    \"num_samples\": -1,\\n    \"seed\": 42\\n}\\n\\nautoml.fit(X_train=X_train, y_train=y_train, **settings)\\n\\n# Close the spark session\\nspark.stop()\\n``` \\n\\nThis code snippet reads a CSV file from spark, converts it into a pandas dataframe, and passes this data to the FLAML `AutoML()` function. FLAML will conduct the hyperparameter tuning and model selection process on this data via the available estimators. The output will show the best settings found for each estimator and their scores. \\n\\nHere, the `use_spark` and `force_cancel` arguments are used for parallel training using Spark and force-cancelling the training if the time limit is reached, respectively.\\n\\nAlso, note that FLAML provides `to_pandas_on_spark` function to convert data structure to Spark compatible format.', 'role': 'user'}\u001b[0m\n",
      "2024-02-26 08:04:16,143 - utils.py:   16 - DEBUG - Inner proxy reply: \u001b[0m\n",
      "2024-02-26 08:04:16,143 - utils.py:   16 - DEBUG - Is update context: False, new message: You can use FLAML for a classification task and train the model in 30 seconds while using Spark to p, length: 1713\u001b[0m\n",
      "2024-02-26 08:04:16,144 - utils.py:   16 - DEBUG - process_message took 0.00 seconds.\u001b[0m\n",
      "2024-02-26 08:04:16,144 - utils.py:   16 - DEBUG - \u001b[32mTo LLM message: \u001b[0m\n",
      "2024-02-26 08:04:16,145 - utils.py:   16 - DEBUG - \u001b[32mTokens in history: 464\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mrag\u001b[0m (to userproxy):\n",
      "\n",
      "You can use FLAML for a classification task and train the model in 30 seconds while using Spark to parallelize the training. Force cancel jobs if the time limit is reached using the following code snippet:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from pyspark.sql import SparkSession\n",
      "from flaml import AutoML\n",
      "\n",
      "spark = SparkSession.builder \\\n",
      "    .appName(\"FLAML\") \\\n",
      "    .config(\"spark.sql.execution.arrow.enabled\",\"true\") \\\n",
      "    .getOrCreate()\n",
      "\n",
      "train_data = spark.read.format(\"csv\").option(\"header\", \"true\").option(\n",
      "    \"inferSchema\", \"true\").load(\"train.csv\")\n",
      "\n",
      "train_pd = train_data.toPandas()\n",
      "X_train = train_pd.drop(columns=['target'])\n",
      "y_train = train_pd['target']\n",
      "\n",
      "automl = AutoML()\n",
      "\n",
      "settings = {\n",
      "    \"time_budget\": 30,\n",
      "    \"metric\": 'accuracy',\n",
      "    \"task\": 'classification',\n",
      "    \"n_jobs\": -1,\n",
      "    \"model_history\": True,\n",
      "    \"log_file_name\": \"log.txt\",\n",
      "    \"estimator_list\": ['lgbm', 'rf', 'xgboost'],\n",
      "    \"use_spark\": True,\n",
      "    \"force_cancel\": True,\n",
      "    \"num_samples\": -1,\n",
      "    \"seed\": 42\n",
      "}\n",
      "\n",
      "automl.fit(X_train=X_train, y_train=y_train, **settings)\n",
      "\n",
      "# Close the spark session\n",
      "spark.stop()\n",
      "``` \n",
      "\n",
      "This code snippet reads a CSV file from spark, converts it into a pandas dataframe, and passes this data to the FLAML `AutoML()` function. FLAML will conduct the hyperparameter tuning and model selection process on this data via the available estimators. The output will show the best settings found for each estimator and their scores. \n",
      "\n",
      "Here, the `use_spark` and `force_cancel` arguments are used for parallel training using Spark and force-cancelling the training if the time limit is reached, respectively.\n",
      "\n",
      "Also, note that FLAML provides `to_pandas_on_spark` function to convert data structure to Spark compatible format.\n",
      "\n",
      "Source: https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Examples/Integrate%20-%20Spark.md\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muserproxy\u001b[0m (to rag):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 08:04:16,149 - utils.py:   16 - DEBUG - History message: {'content': 'How to use FLAML for a classification task and train the model in 30 seconds. Use spark to parallel the training. Force cancel jobs if time limit is reached.', 'role': 'user'}\u001b[0m\n",
      "2024-02-26 08:04:16,149 - utils.py:   16 - DEBUG - History message: {'content': 'You can use FLAML for a classification task and train the model in 30 seconds while using Spark to parallelize the training. Force cancel jobs if the time limit is reached using the following code snippet:\\n\\n```python\\nimport pandas as pd\\nfrom pyspark.sql import SparkSession\\nfrom flaml import AutoML\\n\\nspark = SparkSession.builder \\\\\\n    .appName(\"FLAML\") \\\\\\n    .config(\"spark.sql.execution.arrow.enabled\",\"true\") \\\\\\n    .getOrCreate()\\n\\ntrain_data = spark.read.format(\"csv\").option(\"header\", \"true\").option(\\n    \"inferSchema\", \"true\").load(\"train.csv\")\\n\\ntrain_pd = train_data.toPandas()\\nX_train = train_pd.drop(columns=[\\'target\\'])\\ny_train = train_pd[\\'target\\']\\n\\nautoml = AutoML()\\n\\nsettings = {\\n    \"time_budget\": 30,\\n    \"metric\": \\'accuracy\\',\\n    \"task\": \\'classification\\',\\n    \"n_jobs\": -1,\\n    \"model_history\": True,\\n    \"log_file_name\": \"log.txt\",\\n    \"estimator_list\": [\\'lgbm\\', \\'rf\\', \\'xgboost\\'],\\n    \"use_spark\": True,\\n    \"force_cancel\": True,\\n    \"num_samples\": -1,\\n    \"seed\": 42\\n}\\n\\nautoml.fit(X_train=X_train, y_train=y_train, **settings)\\n\\n# Close the spark session\\nspark.stop()\\n``` \\n\\nThis code snippet reads a CSV file from spark, converts it into a pandas dataframe, and passes this data to the FLAML `AutoML()` function. FLAML will conduct the hyperparameter tuning and model selection process on this data via the available estimators. The output will show the best settings found for each estimator and their scores. \\n\\nHere, the `use_spark` and `force_cancel` arguments are used for parallel training using Spark and force-cancelling the training if the time limit is reached, respectively.\\n\\nAlso, note that FLAML provides `to_pandas_on_spark` function to convert data structure to Spark compatible format.\\n\\nSource: https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Examples/Integrate%20-%20Spark.md', 'role': 'assistant'}\u001b[0m\n",
      "2024-02-26 08:04:16,149 - utils.py:   16 - DEBUG - History message: {'content': 'You can use FLAML for a classification task and train the model in 30 seconds while using Spark to parallelize the training. Force cancel jobs if the time limit is reached using the following code snippet:\\n\\n```python\\nimport pandas as pd\\nfrom pyspark.sql import SparkSession\\nfrom flaml import AutoML\\n\\nspark = SparkSession.builder \\\\\\n    .appName(\"FLAML\") \\\\\\n    .config(\"spark.sql.execution.arrow.enabled\",\"true\") \\\\\\n    .getOrCreate()\\n\\ntrain_data = spark.read.format(\"csv\").option(\"header\", \"true\").option(\\n    \"inferSchema\", \"true\").load(\"train.csv\")\\n\\ntrain_pd = train_data.toPandas()\\nX_train = train_pd.drop(columns=[\\'target\\'])\\ny_train = train_pd[\\'target\\']\\n\\nautoml = AutoML()\\n\\nsettings = {\\n    \"time_budget\": 30,\\n    \"metric\": \\'accuracy\\',\\n    \"task\": \\'classification\\',\\n    \"n_jobs\": -1,\\n    \"model_history\": True,\\n    \"log_file_name\": \"log.txt\",\\n    \"estimator_list\": [\\'lgbm\\', \\'rf\\', \\'xgboost\\'],\\n    \"use_spark\": True,\\n    \"force_cancel\": True,\\n    \"num_samples\": -1,\\n    \"seed\": 42\\n}\\n\\nautoml.fit(X_train=X_train, y_train=y_train, **settings)\\n\\n# Close the spark session\\nspark.stop()\\n``` \\n\\nThis code snippet reads a CSV file from spark, converts it into a pandas dataframe, and passes this data to the FLAML `AutoML()` function. FLAML will conduct the hyperparameter tuning and model selection process on this data via the available estimators. The output will show the best settings found for each estimator and their scores. \\n\\nHere, the `use_spark` and `force_cancel` arguments are used for parallel training using Spark and force-cancelling the training if the time limit is reached, respectively.\\n\\nAlso, note that FLAML provides `to_pandas_on_spark` function to convert data structure to Spark compatible format.\\n\\nSource: https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Examples/Integrate%20-%20Spark.md', 'role': 'assistant'}\u001b[0m\n",
      "2024-02-26 08:04:16,151 - utils.py:   16 - DEBUG - is_code_execution_result: False\u001b[0m\n",
      "2024-02-26 08:04:16,151 - utils.py:   16 - DEBUG - \u001b[32mInput message: \u001b[0m\n",
      "2024-02-26 08:04:16,152 - utils.py:   16 - DEBUG - \u001b[32mReceived raw message: How to use FLAML for a classification task and train the model in 30 seconds. Use spark to parallel the training. Force cancel jobs if time limit is reached.\u001b[0m\n",
      "2024-02-26 08:04:16,152 - utils.py:   16 - DEBUG - Is update context: False, new message: , length: 0\u001b[0m\n",
      "2024-02-26 08:04:16,152 - utils.py:   16 - DEBUG - process_message took 0.00 seconds.\u001b[0m\n",
      "2024-02-26 08:04:16,153 - utils.py:   16 - DEBUG - \u001b[32mTo LLM message: \u001b[0m\n",
      "2024-02-26 08:04:16,153 - utils.py:   16 - DEBUG - \u001b[32mTokens in history: 478\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "code_problem = \"How to use FLAML for a classification task and train the model in 30 seconds. Use spark to parallel the training. Force cancel jobs if time limit is reached.\"\n",
    "rag.rag_filter_document = {\"$contains\": \"spark\"}  # filter documents that contain \"spark\"\n",
    "_ = userproxy.initiate_chat(rag, message=code_problem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "[Back to top](#table-of-contents)\n",
    "\n",
    "Use RagAgent to answer a multi-hop question.\n",
    "\n",
    "Problem: The common authors of the paper FLAML and paper AutoGen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muserproxy\u001b[0m (to rag):\n",
      "\n",
      "The common authors of the paper FLAML and paper AutoGen.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 08:04:16,162 - utils.py:   16 - DEBUG - is_code_execution_result: False\u001b[0m\n",
      "2024-02-26 08:04:16,163 - utils.py:   16 - DEBUG - \u001b[32mInput message: The common authors of the paper FLAML and paper AutoGen.\u001b[0m\n",
      "2024-02-26 08:04:16,163 - utils.py:   16 - DEBUG - \u001b[32mReceived raw message: The common authors of the paper FLAML and paper AutoGen.\u001b[0m\n",
      "2024-02-26 08:04:16,164 - utils.py:   16 - DEBUG - \u001b[32mPerforming RAG for message: The common authors of the paper FLAML and paper AutoGen.\u001b[0m\n",
      "2024-02-26 08:04:16,194 - utils.py:   16 - DEBUG - \u001b[32mTask predicted: multihop\u001b[0m\n",
      "2024-02-26 08:04:16,194 - utils.py:   16 - DEBUG - __call__ took 0.03 seconds.\u001b[0m\n",
      "2024-02-26 08:04:16,195 - utils.py:   16 - DEBUG - \u001b[32mRefined message for db query: ['Who are the authors of the paper FLAML', 'Who are the authors of the paper AutoGen', 'Are there any authors common to both papers']\u001b[0m\n",
      "2024-02-26 08:04:16,284 - utils.py:   16 - DEBUG - retrieve_docs took 0.09 seconds.\u001b[0m\n",
      "2024-02-26 08:04:16,287 - utils.py:   16 - DEBUG - get_docs_by_ids took 0.00 seconds.\u001b[0m\n",
      "2024-02-26 08:04:16,304 - utils.py:   16 - DEBUG - rerank took 0.02 seconds.\u001b[0m\n",
      "2024-02-26 08:04:16,307 - utils.py:   16 - DEBUG - get_docs_by_ids took 0.00 seconds.\u001b[0m\n",
      "2024-02-26 08:04:16,308 - utils.py:   16 - DEBUG - convert_get_results_to_query_results took 0.00 seconds.\u001b[0m\n",
      "2024-02-26 08:04:16,309 - utils.py:   16 - DEBUG - perform_rag took 0.15 seconds.\u001b[0m\n",
      "2024-02-26 08:04:16,310 - utils.py:   16 - DEBUG - \u001b[32mUsed doc ids: set()\u001b[0m\n",
      "2024-02-26 08:04:16,312 - utils.py:   16 - DEBUG - \u001b[32mAdding doc_id 7479265190117655389 to context.\u001b[0m\n",
      "2024-02-26 08:04:16,313 - utils.py:   16 - DEBUG - \u001b[32mAdding doc_id 1546801265083412558 to context.\u001b[0m\n",
      "2024-02-26 08:04:16,315 - utils.py:   16 - DEBUG - \u001b[32mAdding doc_id 8240521285225588758 to context.\u001b[0m\n",
      "2024-02-26 08:04:16,317 - utils.py:   16 - DEBUG - \u001b[32mAdding doc_id 7935924229625607767 to context.\u001b[0m\n",
      "2024-02-26 08:04:16,318 - utils.py:   16 - DEBUG - process_message took 0.15 seconds.\u001b[0m\n",
      "2024-02-26 08:04:16,319 - utils.py:   16 - DEBUG - \u001b[32mTo LLM message: You're a helpful AI assistant with retrieval augmented generation capability.\n",
      "You answer user's ques\u001b[0m\n",
      "2024-02-26 08:04:16,320 - utils.py:   16 - DEBUG - \u001b[32mTokens in history: 0\u001b[0m\n",
      "2024-02-26 08:04:16,336 - utils.py:   16 - DEBUG - Inner LLM reply: {'content': 'The common authors of the paper FLAML and paper AutoGen are Qingyun Wu, Erkang Zhu, and Chi Wang.', 'role': 'user'}\u001b[0m\n",
      "2024-02-26 08:04:16,340 - utils.py:   16 - DEBUG - Inner proxy reply: \u001b[0m\n",
      "2024-02-26 08:04:16,340 - utils.py:   16 - DEBUG - Is update context: False, new message: The common authors of the paper FLAML and paper AutoGen are Qingyun Wu, Erkang Zhu, and Chi Wang., length: 97\u001b[0m\n",
      "2024-02-26 08:04:16,340 - utils.py:   16 - DEBUG - process_message took 0.00 seconds.\u001b[0m\n",
      "2024-02-26 08:04:16,341 - utils.py:   16 - DEBUG - \u001b[32mTo LLM message: \u001b[0m\n",
      "2024-02-26 08:04:16,341 - utils.py:   16 - DEBUG - \u001b[32mTokens in history: 52\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mrag\u001b[0m (to userproxy):\n",
      "\n",
      "The common authors of the paper FLAML and paper AutoGen are Qingyun Wu, Erkang Zhu, and Chi Wang.\n",
      "\n",
      "Source: /datadrive/autogen/notebook/../website/docs/Ecosystem.md https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Research.md /datadrive/autogen/notebook/../website/docs/Research.md\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muserproxy\u001b[0m (to rag):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 08:04:16,344 - utils.py:   16 - DEBUG - History message: {'content': 'The common authors of the paper FLAML and paper AutoGen.', 'role': 'user'}\u001b[0m\n",
      "2024-02-26 08:04:16,344 - utils.py:   16 - DEBUG - History message: {'content': 'The common authors of the paper FLAML and paper AutoGen are Qingyun Wu, Erkang Zhu, and Chi Wang.\\n\\nSource: /datadrive/autogen/notebook/../website/docs/Ecosystem.md https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Research.md /datadrive/autogen/notebook/../website/docs/Research.md', 'role': 'assistant'}\u001b[0m\n",
      "2024-02-26 08:04:16,345 - utils.py:   16 - DEBUG - is_code_execution_result: False\u001b[0m\n",
      "2024-02-26 08:04:16,346 - utils.py:   16 - DEBUG - \u001b[32mInput message: \u001b[0m\n",
      "2024-02-26 08:04:16,346 - utils.py:   16 - DEBUG - \u001b[32mReceived raw message: The common authors of the paper FLAML and paper AutoGen.\u001b[0m\n",
      "2024-02-26 08:04:16,344 - utils.py:   16 - DEBUG - History message: {'content': 'The common authors of the paper FLAML and paper AutoGen are Qingyun Wu, Erkang Zhu, and Chi Wang.\\n\\nSource: /datadrive/autogen/notebook/../website/docs/Ecosystem.md https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Research.md /datadrive/autogen/notebook/../website/docs/Research.md', 'role': 'assistant'}\u001b[0m\n",
      "2024-02-26 08:04:16,345 - utils.py:   16 - DEBUG - is_code_execution_result: False\u001b[0m\n",
      "2024-02-26 08:04:16,346 - utils.py:   16 - DEBUG - \u001b[32mInput message: \u001b[0m\n",
      "2024-02-26 08:04:16,346 - utils.py:   16 - DEBUG - \u001b[32mReceived raw message: The common authors of the paper FLAML and paper AutoGen.\u001b[0m\n",
      "2024-02-26 08:04:16,346 - utils.py:   16 - DEBUG - Is update context: False, new message: , length: 0\u001b[0m\n",
      "2024-02-26 08:04:16,347 - utils.py:   16 - DEBUG - process_message took 0.00 seconds.\u001b[0m\n",
      "2024-02-26 08:04:16,347 - utils.py:   16 - DEBUG - \u001b[32mTo LLM message: \u001b[0m\n",
      "2024-02-26 08:04:16,347 - utils.py:   16 - DEBUG - \u001b[32mTokens in history: 90\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# reset the assistant. Always reset the assistant before starting a new conversation.\n",
    "rag.reset()\n",
    "\n",
    "qa_problem = \"The common authors of the paper FLAML and paper AutoGen.\"\n",
    "rag.rag_filter_document = None  # clear filter\n",
    "_ = userproxy.initiate_chat(rag, message=qa_problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3\n",
    "\n",
    "[Back to top](#table-of-contents)\n",
    "\n",
    "Enable RAG capability for any conversable agent.\n",
    "\n",
    "First, let's try to answer the same question in [Example 2](#example-2) with a default `AssistantAgent`. It couldn't answer it well because AutoGen paper is not in the LLM model's training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_assistant = autogen.AssistantAgent(name=\"normal assistant\", llm_config=llm_config, max_consecutive_auto_reply=3)\n",
    "message = \"The common authors of the paper FLAML and paper AutoGen.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muserproxy\u001b[0m (to normal assistant):\n",
      "\n",
      "The common authors of the paper FLAML and paper AutoGen.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "The common authors of the paper FLAML and paper AutoGen.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mnormal assistant\u001b[0m (to userproxy):\n",
      "\n",
      "Plan:\n",
      "1. Search for the full names of the authors of paper FLAML and paper AutoGen.\n",
      "2. Collect the common authors of the two papers.\n",
      "\n",
      "Code:\n",
      "```python\n",
      "# Search for the full names of the authors of paper FLAML and paper AutoGen.\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "def search_authors(paper_title):\n",
      "    base_url = \"https://scholar.google.com/\"\n",
      "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
      "    query = \"q={}\".format(paper_title)\n",
      "    \n",
      "    url = base_url + \"scholar?\" + query\n",
      "    page = requests.get(url, headers=headers)\n",
      "    soup = BeautifulSoup(page.content, 'html.parser')\n",
      "    \n",
      "    author_tags = soup.select('.gs_a')\n",
      "    authors = []\n",
      "    for tag in author_tags:\n",
      "        author = tag.text.split(' -')[0]\n",
      "        authors.append(author)\n",
      "        \n",
      "    return authors\n",
      "\n",
      "flaml_authors = search_authors(\"FLAML: A Python Framework for Ultra-Fast Machine Learning\")\n",
      "autogen_authors = search_authors(\"AutoGen: Automated Feature Generation and Selection\")\n",
      "\n",
      "# Collect the common authors of the two papers.\n",
      "common_authors = set(flaml_authors).intersection(autogen_authors)\n",
      "print(common_authors)\n",
      "```\n",
      "\n",
      "Expected Output:\n",
      "```\n",
      "{'Qingyun Wu', 'Randal S. Olson'}\n",
      "```\n",
      "\n",
      "Note: The output may vary depending on the current state of the Google Scholar search results.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muserproxy\u001b[0m (to normal assistant):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mnormal assistant\u001b[0m (to userproxy):\n",
      "\n",
      "Do you have any other task to be done?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muserproxy\u001b[0m (to normal assistant):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mnormal assistant\u001b[0m (to userproxy):\n",
      "\n",
      "Sure, feel free to ask me any task.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muserproxy\u001b[0m (to normal assistant):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "normal_assistant.reset()\n",
    "\n",
    "_ = userproxy.initiate_chat(normal_assistant, message=message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, let's enable `Ragability` for the same assistant agent. As you can see, with the `Ragability` the agent can now answer the question correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 08:04:16,705 - utils.py:   19 - INFO - \u001b[32mUse the existing collection `autogen-rag`.\u001b[0m\n",
      "2024-02-26 08:04:16,707 - utils.py:   16 - DEBUG - insert_docs took 0.00 seconds.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muserproxy\u001b[0m (to normal assistant):\n",
      "\n",
      "The common authors of the paper FLAML and paper AutoGen.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mnormal assistant\u001b[0m (to rag_agent):\n",
      "\n",
      "The common authors of the paper FLAML and paper AutoGen.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 08:04:16,736 - utils.py:   16 - DEBUG - is_code_execution_result: False\u001b[0m\n",
      "2024-02-26 08:04:16,737 - utils.py:   16 - DEBUG - \u001b[32mInput message: The common authors of the paper FLAML and paper AutoGen.\u001b[0m\n",
      "2024-02-26 08:04:16,737 - utils.py:   16 - DEBUG - \u001b[32mReceived raw message: The common authors of the paper FLAML and paper AutoGen.\u001b[0m\n",
      "2024-02-26 08:04:16,738 - utils.py:   16 - DEBUG - \u001b[32mPerforming RAG for message: The common authors of the paper FLAML and paper AutoGen.\u001b[0m\n",
      "2024-02-26 08:04:16,766 - utils.py:   16 - DEBUG - \u001b[32mTask predicted: multihop\u001b[0m\n",
      "2024-02-26 08:04:16,766 - utils.py:   16 - DEBUG - __call__ took 0.03 seconds.\u001b[0m\n",
      "2024-02-26 08:04:16,767 - utils.py:   16 - DEBUG - \u001b[32mRefined message for db query: ['Who are the authors of the paper FLAML', 'Who are the authors of the paper AutoGen', 'Are there any authors common to both papers']\u001b[0m\n",
      "2024-02-26 08:04:16,837 - utils.py:   16 - DEBUG - retrieve_docs took 0.07 seconds.\u001b[0m\n",
      "2024-02-26 08:04:16,840 - utils.py:   16 - DEBUG - get_docs_by_ids took 0.00 seconds.\u001b[0m\n",
      "2024-02-26 08:04:16,850 - utils.py:   16 - DEBUG - rerank took 0.01 seconds.\u001b[0m\n",
      "2024-02-26 08:04:16,852 - utils.py:   16 - DEBUG - get_docs_by_ids took 0.00 seconds.\u001b[0m\n",
      "2024-02-26 08:04:16,853 - utils.py:   16 - DEBUG - convert_get_results_to_query_results took 0.00 seconds.\u001b[0m\n",
      "2024-02-26 08:04:16,854 - utils.py:   16 - DEBUG - perform_rag took 0.12 seconds.\u001b[0m\n",
      "2024-02-26 08:04:16,854 - utils.py:   16 - DEBUG - \u001b[32mUsed doc ids: set()\u001b[0m\n",
      "2024-02-26 08:04:16,856 - utils.py:   16 - DEBUG - \u001b[32mAdding doc_id 7479265190117655389 to context.\u001b[0m\n",
      "2024-02-26 08:04:16,858 - utils.py:   16 - DEBUG - \u001b[32mAdding doc_id 1546801265083412558 to context.\u001b[0m\n",
      "2024-02-26 08:04:16,859 - utils.py:   16 - DEBUG - \u001b[32mAdding doc_id 8240521285225588758 to context.\u001b[0m\n",
      "2024-02-26 08:04:16,860 - utils.py:   16 - DEBUG - \u001b[32mAdding doc_id 7935924229625607767 to context.\u001b[0m\n",
      "2024-02-26 08:04:16,865 - utils.py:   16 - DEBUG - process_message took 0.13 seconds.\u001b[0m\n",
      "2024-02-26 08:04:16,866 - utils.py:   16 - DEBUG - \u001b[32mTo LLM message: You're a helpful AI assistant with retrieval augmented generation capability.\n",
      "You answer user's ques\u001b[0m\n",
      "2024-02-26 08:04:16,866 - utils.py:   16 - DEBUG - \u001b[32mTokens in history: 0\u001b[0m\n",
      "2024-02-26 08:04:16,889 - utils.py:   16 - DEBUG - Inner LLM reply: {'content': 'The common authors of the paper FLAML and paper AutoGen are Qingyun Wu, Erkang Zhu, and Chi Wang.', 'role': 'user'}\u001b[0m\n",
      "2024-02-26 08:04:16,893 - utils.py:   16 - DEBUG - Inner proxy reply: \u001b[0m\n",
      "2024-02-26 08:04:16,894 - utils.py:   16 - DEBUG - Is update context: False, new message: The common authors of the paper FLAML and paper AutoGen are Qingyun Wu, Erkang Zhu, and Chi Wang., length: 97\u001b[0m\n",
      "2024-02-26 08:04:16,894 - utils.py:   16 - DEBUG - process_message took 0.00 seconds.\u001b[0m\n",
      "2024-02-26 08:04:16,894 - utils.py:   16 - DEBUG - \u001b[32mTo LLM message: \u001b[0m\n",
      "2024-02-26 08:04:16,895 - utils.py:   16 - DEBUG - \u001b[32mTokens in history: 52\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mrag_agent\u001b[0m (to normal assistant):\n",
      "\n",
      "The common authors of the paper FLAML and paper AutoGen are Qingyun Wu, Erkang Zhu, and Chi Wang.\n",
      "\n",
      "Source: /datadrive/autogen/notebook/../website/docs/Ecosystem.md https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Research.md /datadrive/autogen/notebook/../website/docs/Research.md\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "The common authors of the paper FLAML and paper AutoGen are Qingyun Wu, Erkang Zhu, and Chi Wang.\n",
      "\n",
      "Source: /datadrive/autogen/notebook/../website/docs/Ecosystem.md https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Research.md /datadrive/autogen/notebook/../website/docs/Research.md\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mnormal assistant\u001b[0m (to userproxy):\n",
      "\n",
      "The common authors of FLAML and AutoGen papers are Qingyun Wu, Erkang Zhu, and Chi Wang. This information was retrieved from the following sources:\n",
      "\n",
      "1. /datadrive/autogen/notebook/../website/docs/Ecosystem.md \n",
      "2. https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Research.md\n",
      "3. /datadrive/autogen/notebook/../website/docs/Research.md\n",
      "\n",
      "Is there anything else I can help you with?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muserproxy\u001b[0m (to normal assistant):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mnormal assistant\u001b[0m (to userproxy):\n",
      "\n",
      "Is there anything I can help you with?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muserproxy\u001b[0m (to normal assistant):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mnormal assistant\u001b[0m (to userproxy):\n",
      "\n",
      "Please let me know if you have any questions or tasks that I can help you with.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muserproxy\u001b[0m (to normal assistant):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "rag_config[\"overwrite\"] = False\n",
    "ragability = Ragability(llm_config=llm_config, rag_config=rag_config, verbose=2)\n",
    "ragability.add_to_agent(normal_assistant)\n",
    "\n",
    "normal_assistant.reset()\n",
    "\n",
    "_ = userproxy.initiate_chat(normal_assistant, message=message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
