{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "tags: [\"RAG\"]\n",
    "description: |\n",
    "    Explore the use of AutoGen's RagAgent for tasks like code generation from docstrings, answering complex questions with human feedback, and exploiting features like Update Context, custom prompts, and few-shot learning.\n",
    "-->\n",
    "\n",
    "# Using RagAgent for Retrieval-Augmented Code Generation and Question Answering\n",
    "\n",
    "Introducing an agent capable of performing Retrieval-Augmented Generation (RAG) for the given message.\n",
    "\n",
    "Upon receipt of a message, the agent employs RAG to generate a reply. It retrieves documents based on the message, then generates a reply using both the retrieved documents and the message itself. Additionally, it supports automatic context updates during the conversation, either autonomously or at the user`s request.\n",
    "\n",
    "## Table of Contents\n",
    "We'll demonstrate six examples of using RetrieveChat for code generation and question answering:\n",
    "\n",
    "- [Example 1: Use RagAgent to help generate code](#example-1)\n",
    "- [Example 2: Use RagAgent to answer a multi-hop question.](#example-2)\n",
    "\n",
    "\n",
    "````{=mdx}\n",
    ":::info Requirements\n",
    "Some extra dependencies are needed for this notebook, which can be installed via pip:\n",
    "\n",
    "```bash\n",
    "pip install pyautogen[rag] flaml[automl]\n",
    "```\n",
    "\n",
    "For more information, please refer to the [installation guide](/docs/installation/).\n",
    ":::\n",
    "````"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "The [`config_list_from_json`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models to use:  ['gpt-35-turbo', 'gpt-35-turbo-0613']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import logging\n",
    "import autogen\n",
    "from autogen.agentchat.contrib.rag import RagAgent, logger\n",
    "from autogen.agentchat.contrib.rag.splitter import SUPPORTED_EXTENSIONS\n",
    "\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "config_list = autogen.config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\")\n",
    "\n",
    "assert len(config_list) > 0\n",
    "print(\"models to use: \", [config_list[i][\"model\"] for i in range(len(config_list))])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{=mdx}\n",
    ":::tip\n",
    "Learn more about configuring LLMs for agents [here](/docs/llm_configuration).\n",
    ":::\n",
    "````\n",
    "\n",
    "## Construct agents for RAG\n",
    "\n",
    "We start by initializing the `RagAgent` and `UserProxyAgent`. The `RagAgent` will be responsible for replying to `UserProxyAgent` with retrieval-augmented generation based on the given knowledge base contained in the `docs_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepted file formats for `docs_path`:\n",
      "['.txt', '.json', '.csv', '.tsv', '.md', '.html', '.htm', '.rtf', '.rst', '.jsonl', '.log', '.xml', '.yaml', '.yml', '.pdf']\n"
     ]
    }
   ],
   "source": [
    "print(\"Accepted file formats for `docs_path`:\")\n",
    "print(SUPPORTED_EXTENSIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 04:24:10,323 - utils.py:   16 - DEBUG - Processing file: ./tmp/download/Integrate%20-%20Spark.md\u001b[0m\n",
      "2024-02-26 04:24:10,326 - utils.py:   16 - DEBUG - \u001b[32mSplit ./tmp/download/Integrate%20-%20Spark.md into 2 chunks.\u001b[0m\n",
      "2024-02-26 04:24:10,327 - utils.py:   16 - DEBUG - Processing file: ./tmp/download/Research.md\u001b[0m\n",
      "2024-02-26 04:24:10,329 - utils.py:   16 - DEBUG - \u001b[32mSplit ./tmp/download/Research.md into 2 chunks.\u001b[0m\n",
      "2024-02-26 04:24:10,330 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/Research.md\u001b[0m\n",
      "2024-02-26 04:24:10,331 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/Research.md into 1 chunks.\u001b[0m\n",
      "2024-02-26 04:24:10,332 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/Contribute.md\u001b[0m\n",
      "2024-02-26 04:24:10,326 - utils.py:   16 - DEBUG - \u001b[32mSplit ./tmp/download/Integrate%20-%20Spark.md into 2 chunks.\u001b[0m\n",
      "2024-02-26 04:24:10,327 - utils.py:   16 - DEBUG - Processing file: ./tmp/download/Research.md\u001b[0m\n",
      "2024-02-26 04:24:10,329 - utils.py:   16 - DEBUG - \u001b[32mSplit ./tmp/download/Research.md into 2 chunks.\u001b[0m\n",
      "2024-02-26 04:24:10,330 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/Research.md\u001b[0m\n",
      "2024-02-26 04:24:10,331 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/Research.md into 1 chunks.\u001b[0m\n",
      "2024-02-26 04:24:10,332 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/Contribute.md\u001b[0m\n",
      "2024-02-26 04:24:10,337 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/Contribute.md into 3 chunks.\u001b[0m\n",
      "2024-02-26 04:24:10,337 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/Ecosystem.md\u001b[0m\n",
      "2024-02-26 04:24:10,338 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/Ecosystem.md into 1 chunks.\u001b[0m\n",
      "2024-02-26 04:24:10,339 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/Examples.md\u001b[0m\n",
      "2024-02-26 04:24:10,343 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/Examples.md into 3 chunks.\u001b[0m\n",
      "2024-02-26 04:24:10,343 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/FAQ.md\u001b[0m\n",
      "2024-02-26 04:24:10,348 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/FAQ.md into 4 chunks.\u001b[0m\n",
      "2024-02-26 04:24:10,349 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/Getting-Started.md\u001b[0m\n",
      "2024-02-26 04:24:10,351 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/Getting-Started.md into 2 chunks.\u001b[0m\n",
      "2024-02-26 04:24:10,352 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/Migration-Guide.md\u001b[0m\n",
      "2024-02-26 04:24:10,353 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/Migration-Guide.md into 1 chunks.\u001b[0m\n",
      "2024-02-26 04:24:10,354 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/Use-Cases/agent_chat.md\u001b[0m\n",
      "2024-02-26 04:24:10,361 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/Use-Cases/agent_chat.md into 6 chunks.\u001b[0m\n",
      "2024-02-26 04:24:10,361 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/Use-Cases/enhanced_inference.md\u001b[0m\n",
      "2024-02-26 04:24:10,370 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/Use-Cases/enhanced_inference.md into 6 chunks.\u001b[0m\n",
      "2024-02-26 04:24:10,371 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/installation/Docker.md\u001b[0m\n",
      "2024-02-26 04:24:10,373 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/installation/Docker.md into 2 chunks.\u001b[0m\n",
      "2024-02-26 04:24:10,374 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/installation/Optional-Dependencies.md\u001b[0m\n",
      "2024-02-26 04:24:10,376 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/installation/Optional-Dependencies.md into 1 chunks.\u001b[0m\n",
      "2024-02-26 04:24:10,376 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/Research.md\u001b[0m\n",
      "2024-02-26 04:24:10,378 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/Research.md into 1 chunks.\u001b[0m\n",
      "2024-02-26 04:24:10,378 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/Contribute.md\u001b[0m\n",
      "2024-02-26 04:24:10,383 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/Contribute.md into 3 chunks.\u001b[0m\n",
      "2024-02-26 04:24:10,383 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/Ecosystem.md\u001b[0m\n",
      "2024-02-26 04:24:10,385 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/Ecosystem.md into 1 chunks.\u001b[0m\n",
      "2024-02-26 04:24:10,385 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/Examples.md\u001b[0m\n",
      "2024-02-26 04:24:10,389 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/Examples.md into 3 chunks.\u001b[0m\n",
      "2024-02-26 04:24:10,389 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/FAQ.md\u001b[0m\n",
      "2024-02-26 04:24:10,394 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/FAQ.md into 4 chunks.\u001b[0m\n",
      "2024-02-26 04:24:10,395 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/Getting-Started.md\u001b[0m\n",
      "2024-02-26 04:24:10,398 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/Getting-Started.md into 2 chunks.\u001b[0m\n",
      "2024-02-26 04:24:10,398 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/Migration-Guide.md\u001b[0m\n",
      "2024-02-26 04:24:10,400 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/Migration-Guide.md into 1 chunks.\u001b[0m\n",
      "2024-02-26 04:24:10,400 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/Use-Cases/agent_chat.md\u001b[0m\n",
      "2024-02-26 04:24:10,407 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/Use-Cases/agent_chat.md into 6 chunks.\u001b[0m\n",
      "2024-02-26 04:24:10,408 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/Use-Cases/enhanced_inference.md\u001b[0m\n",
      "2024-02-26 04:24:10,417 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/Use-Cases/enhanced_inference.md into 6 chunks.\u001b[0m\n",
      "2024-02-26 04:24:10,418 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/installation/Docker.md\u001b[0m\n",
      "2024-02-26 04:24:10,420 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/installation/Docker.md into 2 chunks.\u001b[0m\n",
      "2024-02-26 04:24:10,420 - utils.py:   16 - DEBUG - Processing file: /datadrive/autogen/notebook/../website/docs/installation/Optional-Dependencies.md\u001b[0m\n",
      "2024-02-26 04:24:10,423 - utils.py:   16 - DEBUG - \u001b[32mSplit /datadrive/autogen/notebook/../website/docs/installation/Optional-Dependencies.md into 1 chunks.\u001b[0m\n",
      "2024-02-26 04:24:10,424 - utils.py:   16 - DEBUG - split took 0.16 seconds.\u001b[0m\n",
      "2024-02-26 04:24:12,680 - utils.py:   16 - DEBUG - encode_chunks took 2.25 seconds.\u001b[0m\n",
      "2024-02-26 04:24:12,810 - utils.py:   16 - DEBUG - insert_docs took 0.07 seconds.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "llm_config = {\n",
    "    \"timeout\": 60,\n",
    "    \"config_list\": config_list,\n",
    "    # \"cache_seed\": None,  # set to None if you want to disable caching\n",
    "}\n",
    "\n",
    "\n",
    "def termination_msg(x):\n",
    "    return isinstance(x, dict) and \"TERMINATE\" == str(x.get(\"content\", \"\"))[-9:].upper()\n",
    "\n",
    "\n",
    "userproxy = autogen.UserProxyAgent(\n",
    "    name=\"userproxy\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config=False,  # {\"use_docker\":False, \"work_dir\":\"./tmp\"},\n",
    "    # default_auto_reply=\"Reply `TERMINATE` if the task is done.\",\n",
    "    description=\"The boss who ask questions and give tasks.\",\n",
    ")\n",
    "\n",
    "# Check the docstring of RagAgent for more details on the configuration.\n",
    "rag_config = {\n",
    "    \"docs_path\": [\n",
    "        \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Examples/Integrate%20-%20Spark.md\",\n",
    "        \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Research.md\",\n",
    "        os.path.join(os.path.abspath(\"\"), \"..\", \"website\", \"docs\"),\n",
    "    ],\n",
    "    \"overwrite\": True,  # set to False to avoid overwriting the existing documents\n",
    "}\n",
    "\n",
    "rag = RagAgent(\n",
    "    name=\"rag\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=5,\n",
    "    llm_config=llm_config,\n",
    "    rag_config=rag_config,\n",
    "    code_execution_config=False,\n",
    "    description=\"Assistant that can answer questions and generate code based on retrieved documents.\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "\n",
    "[Back to top](#table-of-contents)\n",
    "\n",
    "Use RagAgent to help generate sample code and automatically run the code and fix errors if there is any.\n",
    "\n",
    "Problem: How to use FLAML for a classification task and train the model in 30 seconds. Use spark to parallel the training. Force cancel jobs if time limit is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muserproxy\u001b[0m (to rag):\n",
      "\n",
      "How to use FLAML for a classification task and train the model in 30 seconds. Use spark to parallel the training. Force cancel jobs if time limit is reached.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 04:24:12,846 - utils.py:   16 - DEBUG - is_code_execution_result: False\u001b[0m\n",
      "2024-02-26 04:24:12,846 - utils.py:   16 - DEBUG - \u001b[32mInput message: How to use FLAML for a classification task and train the model in 30 seconds. Use spark to parallel the training. Force cancel jobs if time limit is reached.\u001b[0m\n",
      "2024-02-26 04:24:12,847 - utils.py:   16 - DEBUG - \u001b[32mReceived raw message: How to use FLAML for a classification task and train the model in 30 seconds. Use spark to parallel the training. Force cancel jobs if time limit is reached.\u001b[0m\n",
      "2024-02-26 04:24:12,847 - utils.py:   16 - DEBUG - \u001b[32mPerforming RAG for message: How to use FLAML for a classification task and train the model in 30 seconds. Use spark to parallel the training. Force cancel jobs if time limit is reached.\u001b[0m\n",
      "2024-02-26 04:24:13,636 - utils.py:   16 - DEBUG - \u001b[32mTask predicted: `code`\u001b[0m\n",
      "2024-02-26 04:24:13,637 - utils.py:   16 - DEBUG - __call__ took 0.79 seconds.\u001b[0m\n",
      "2024-02-26 04:24:13,637 - utils.py:   16 - DEBUG - \u001b[32mRefined message for db query: ['How to use FLAML for a classification task and train the model in 30 seconds', 'How to parallelize FLAML training using Spark and cancel jobs if the time limit is reached']\u001b[0m\n",
      "2024-02-26 04:24:13,698 - utils.py:   16 - DEBUG - retrieve_docs took 0.06 seconds.\u001b[0m\n",
      "2024-02-26 04:24:13,701 - utils.py:   16 - DEBUG - get_docs_by_ids took 0.00 seconds.\u001b[0m\n",
      "2024-02-26 04:24:13,706 - utils.py:   16 - DEBUG - rerank took 0.00 seconds.\u001b[0m\n",
      "2024-02-26 04:24:13,708 - utils.py:   16 - DEBUG - get_docs_by_ids took 0.00 seconds.\u001b[0m\n",
      "2024-02-26 04:24:13,709 - utils.py:   16 - DEBUG - convert_get_results_to_query_results took 0.00 seconds.\u001b[0m\n",
      "2024-02-26 04:24:13,710 - utils.py:   16 - DEBUG - perform_rag took 0.86 seconds.\u001b[0m\n",
      "2024-02-26 04:24:13,710 - utils.py:   16 - DEBUG - \u001b[32mUsed doc ids: set()\u001b[0m\n",
      "2024-02-26 04:24:13,712 - utils.py:   16 - DEBUG - \u001b[32mAdding doc_id 6609531030114356304 to context.\u001b[0m\n",
      "2024-02-26 04:24:13,714 - utils.py:   16 - DEBUG - \u001b[32mAdding doc_id 6662611751040089415 to context.\u001b[0m\n",
      "2024-02-26 04:24:13,714 - utils.py:   16 - DEBUG - process_message took 0.87 seconds.\u001b[0m\n",
      "2024-02-26 04:24:13,715 - utils.py:   16 - DEBUG - \u001b[32mTo LLM message: You're a retrieval augmented coding assistant. You answer user's questions based on your own knowled\u001b[0m\n",
      "2024-02-26 04:24:13,716 - utils.py:   16 - DEBUG - \u001b[32mTokens in history: 0\u001b[0m\n",
      "2024-02-26 04:24:17,584 - utils.py:   16 - DEBUG - Inner LLM reply: {'content': 'You can use FLAML for a classification task and train the model in 30 seconds while using Spark to parallelize the training. Force cancel jobs if the time limit is reached using the following code snippet:\\n\\n```python\\nimport pandas as pd\\nfrom pyspark.sql import SparkSession\\nfrom flaml import AutoML\\n\\nspark = SparkSession.builder \\\\\\n    .appName(\"FLAML\") \\\\\\n    .config(\"spark.sql.execution.arrow.enabled\",\"true\") \\\\\\n    .getOrCreate()\\n\\ntrain_data = spark.read.format(\"csv\").option(\"header\", \"true\").option(\\n    \"inferSchema\", \"true\").load(\"train.csv\")\\n\\ntrain_pd = train_data.toPandas()\\nX_train = train_pd.drop(columns=[\\'target\\'])\\ny_train = train_pd[\\'target\\']\\n\\nautoml = AutoML()\\n\\nsettings = {\\n    \"time_budget\": 30,\\n    \"metric\": \\'accuracy\\',\\n    \"task\": \\'classification\\',\\n    \"n_jobs\": -1,\\n    \"model_history\": True,\\n    \"log_file_name\": \"log.txt\",\\n    \"estimator_list\": [\\'lgbm\\', \\'rf\\', \\'xgboost\\'],\\n    \"use_spark\": True,\\n    \"force_cancel\": True,\\n    \"num_samples\": -1,\\n    \"seed\": 42\\n}\\n\\nautoml.fit(X_train=X_train, y_train=y_train, **settings)\\n\\n# Close the spark session\\nspark.stop()\\n``` \\n\\nThis code snippet reads a CSV file from spark, converts it into a pandas dataframe, and passes this data to the FLAML `AutoML()` function. FLAML will conduct the hyperparameter tuning and model selection process on this data via the available estimators. The output will show the best settings found for each estimator and their scores. \\n\\nHere, the `use_spark` and `force_cancel` arguments are used for parallel training using Spark and force-cancelling the training if the time limit is reached, respectively.\\n\\nAlso, note that FLAML provides `to_pandas_on_spark` function to convert data structure to Spark compatible format.', 'role': 'user'}\u001b[0m\n",
      "2024-02-26 04:24:17,588 - utils.py:   16 - DEBUG - Inner proxy reply: \u001b[0m\n",
      "2024-02-26 04:24:17,589 - utils.py:   16 - DEBUG - Is update context: False, new message: You can use FLAML for a classification task and train the model in 30 seconds while using Spark to p, length: 1713\u001b[0m\n",
      "2024-02-26 04:24:17,589 - utils.py:   16 - DEBUG - process_message took 0.00 seconds.\u001b[0m\n",
      "2024-02-26 04:24:17,590 - utils.py:   16 - DEBUG - \u001b[32mTo LLM message: \u001b[0m\n",
      "2024-02-26 04:24:17,590 - utils.py:   16 - DEBUG - \u001b[32mTokens in history: 464\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mrag\u001b[0m (to userproxy):\n",
      "\n",
      "You can use FLAML for a classification task and train the model in 30 seconds while using Spark to parallelize the training. Force cancel jobs if the time limit is reached using the following code snippet:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from pyspark.sql import SparkSession\n",
      "from flaml import AutoML\n",
      "\n",
      "spark = SparkSession.builder \\\n",
      "    .appName(\"FLAML\") \\\n",
      "    .config(\"spark.sql.execution.arrow.enabled\",\"true\") \\\n",
      "    .getOrCreate()\n",
      "\n",
      "train_data = spark.read.format(\"csv\").option(\"header\", \"true\").option(\n",
      "    \"inferSchema\", \"true\").load(\"train.csv\")\n",
      "\n",
      "train_pd = train_data.toPandas()\n",
      "X_train = train_pd.drop(columns=['target'])\n",
      "y_train = train_pd['target']\n",
      "\n",
      "automl = AutoML()\n",
      "\n",
      "settings = {\n",
      "    \"time_budget\": 30,\n",
      "    \"metric\": 'accuracy',\n",
      "    \"task\": 'classification',\n",
      "    \"n_jobs\": -1,\n",
      "    \"model_history\": True,\n",
      "    \"log_file_name\": \"log.txt\",\n",
      "    \"estimator_list\": ['lgbm', 'rf', 'xgboost'],\n",
      "    \"use_spark\": True,\n",
      "    \"force_cancel\": True,\n",
      "    \"num_samples\": -1,\n",
      "    \"seed\": 42\n",
      "}\n",
      "\n",
      "automl.fit(X_train=X_train, y_train=y_train, **settings)\n",
      "\n",
      "# Close the spark session\n",
      "spark.stop()\n",
      "``` \n",
      "\n",
      "This code snippet reads a CSV file from spark, converts it into a pandas dataframe, and passes this data to the FLAML `AutoML()` function. FLAML will conduct the hyperparameter tuning and model selection process on this data via the available estimators. The output will show the best settings found for each estimator and their scores. \n",
      "\n",
      "Here, the `use_spark` and `force_cancel` arguments are used for parallel training using Spark and force-cancelling the training if the time limit is reached, respectively.\n",
      "\n",
      "Also, note that FLAML provides `to_pandas_on_spark` function to convert data structure to Spark compatible format.\n",
      "\n",
      "Source: ./tmp/download/Integrate%20-%20Spark.md\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muserproxy\u001b[0m (to rag):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 04:24:17,594 - utils.py:   16 - DEBUG - History message: {'content': 'How to use FLAML for a classification task and train the model in 30 seconds. Use spark to parallel the training. Force cancel jobs if time limit is reached.', 'role': 'user'}\u001b[0m\n",
      "2024-02-26 04:24:17,594 - utils.py:   16 - DEBUG - History message: {'content': 'You can use FLAML for a classification task and train the model in 30 seconds while using Spark to parallelize the training. Force cancel jobs if the time limit is reached using the following code snippet:\\n\\n```python\\nimport pandas as pd\\nfrom pyspark.sql import SparkSession\\nfrom flaml import AutoML\\n\\nspark = SparkSession.builder \\\\\\n    .appName(\"FLAML\") \\\\\\n    .config(\"spark.sql.execution.arrow.enabled\",\"true\") \\\\\\n    .getOrCreate()\\n\\ntrain_data = spark.read.format(\"csv\").option(\"header\", \"true\").option(\\n    \"inferSchema\", \"true\").load(\"train.csv\")\\n\\ntrain_pd = train_data.toPandas()\\nX_train = train_pd.drop(columns=[\\'target\\'])\\ny_train = train_pd[\\'target\\']\\n\\nautoml = AutoML()\\n\\nsettings = {\\n    \"time_budget\": 30,\\n    \"metric\": \\'accuracy\\',\\n    \"task\": \\'classification\\',\\n    \"n_jobs\": -1,\\n    \"model_history\": True,\\n    \"log_file_name\": \"log.txt\",\\n    \"estimator_list\": [\\'lgbm\\', \\'rf\\', \\'xgboost\\'],\\n    \"use_spark\": True,\\n    \"force_cancel\": True,\\n    \"num_samples\": -1,\\n    \"seed\": 42\\n}\\n\\nautoml.fit(X_train=X_train, y_train=y_train, **settings)\\n\\n# Close the spark session\\nspark.stop()\\n``` \\n\\nThis code snippet reads a CSV file from spark, converts it into a pandas dataframe, and passes this data to the FLAML `AutoML()` function. FLAML will conduct the hyperparameter tuning and model selection process on this data via the available estimators. The output will show the best settings found for each estimator and their scores. \\n\\nHere, the `use_spark` and `force_cancel` arguments are used for parallel training using Spark and force-cancelling the training if the time limit is reached, respectively.\\n\\nAlso, note that FLAML provides `to_pandas_on_spark` function to convert data structure to Spark compatible format.\\n\\nSource: ./tmp/download/Integrate%20-%20Spark.md', 'role': 'assistant'}\u001b[0m\n",
      "2024-02-26 04:24:17,596 - utils.py:   16 - DEBUG - is_code_execution_result: False\u001b[0m\n",
      "2024-02-26 04:24:17,596 - utils.py:   16 - DEBUG - \u001b[32mInput message: \u001b[0m\n",
      "2024-02-26 04:24:17,597 - utils.py:   16 - DEBUG - \u001b[32mReceived raw message: How to use FLAML for a classification task and train the model in 30 seconds. Use spark to parallel the training. Force cancel jobs if time limit is reached.\u001b[0m\n",
      "2024-02-26 04:24:17,597 - utils.py:   16 - DEBUG - Is update context: False, new message: , length: 0\u001b[0m\n",
      "2024-02-26 04:24:17,597 - utils.py:   16 - DEBUG - process_message took 0.00 seconds.\u001b[0m\n",
      "2024-02-26 04:24:17,598 - utils.py:   16 - DEBUG - \u001b[32mTo LLM message: \u001b[0m\n",
      "2024-02-26 04:24:17,599 - utils.py:   16 - DEBUG - \u001b[32mTokens in history: 465\u001b[0m\n",
      "2024-02-26 04:24:17,594 - utils.py:   16 - DEBUG - History message: {'content': 'You can use FLAML for a classification task and train the model in 30 seconds while using Spark to parallelize the training. Force cancel jobs if the time limit is reached using the following code snippet:\\n\\n```python\\nimport pandas as pd\\nfrom pyspark.sql import SparkSession\\nfrom flaml import AutoML\\n\\nspark = SparkSession.builder \\\\\\n    .appName(\"FLAML\") \\\\\\n    .config(\"spark.sql.execution.arrow.enabled\",\"true\") \\\\\\n    .getOrCreate()\\n\\ntrain_data = spark.read.format(\"csv\").option(\"header\", \"true\").option(\\n    \"inferSchema\", \"true\").load(\"train.csv\")\\n\\ntrain_pd = train_data.toPandas()\\nX_train = train_pd.drop(columns=[\\'target\\'])\\ny_train = train_pd[\\'target\\']\\n\\nautoml = AutoML()\\n\\nsettings = {\\n    \"time_budget\": 30,\\n    \"metric\": \\'accuracy\\',\\n    \"task\": \\'classification\\',\\n    \"n_jobs\": -1,\\n    \"model_history\": True,\\n    \"log_file_name\": \"log.txt\",\\n    \"estimator_list\": [\\'lgbm\\', \\'rf\\', \\'xgboost\\'],\\n    \"use_spark\": True,\\n    \"force_cancel\": True,\\n    \"num_samples\": -1,\\n    \"seed\": 42\\n}\\n\\nautoml.fit(X_train=X_train, y_train=y_train, **settings)\\n\\n# Close the spark session\\nspark.stop()\\n``` \\n\\nThis code snippet reads a CSV file from spark, converts it into a pandas dataframe, and passes this data to the FLAML `AutoML()` function. FLAML will conduct the hyperparameter tuning and model selection process on this data via the available estimators. The output will show the best settings found for each estimator and their scores. \\n\\nHere, the `use_spark` and `force_cancel` arguments are used for parallel training using Spark and force-cancelling the training if the time limit is reached, respectively.\\n\\nAlso, note that FLAML provides `to_pandas_on_spark` function to convert data structure to Spark compatible format.\\n\\nSource: ./tmp/download/Integrate%20-%20Spark.md', 'role': 'assistant'}\u001b[0m\n",
      "2024-02-26 04:24:17,596 - utils.py:   16 - DEBUG - is_code_execution_result: False\u001b[0m\n",
      "2024-02-26 04:24:17,596 - utils.py:   16 - DEBUG - \u001b[32mInput message: \u001b[0m\n",
      "2024-02-26 04:24:17,597 - utils.py:   16 - DEBUG - \u001b[32mReceived raw message: How to use FLAML for a classification task and train the model in 30 seconds. Use spark to parallel the training. Force cancel jobs if time limit is reached.\u001b[0m\n",
      "2024-02-26 04:24:17,597 - utils.py:   16 - DEBUG - Is update context: False, new message: , length: 0\u001b[0m\n",
      "2024-02-26 04:24:17,597 - utils.py:   16 - DEBUG - process_message took 0.00 seconds.\u001b[0m\n",
      "2024-02-26 04:24:17,598 - utils.py:   16 - DEBUG - \u001b[32mTo LLM message: \u001b[0m\n",
      "2024-02-26 04:24:17,599 - utils.py:   16 - DEBUG - \u001b[32mTokens in history: 465\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "code_problem = \"How to use FLAML for a classification task and train the model in 30 seconds. Use spark to parallel the training. Force cancel jobs if time limit is reached.\"\n",
    "rag.rag_filter_document = {\"$contains\": \"spark\"}  # filter documents that contain \"spark\"\n",
    "_ = userproxy.initiate_chat(rag, message=code_problem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "[Back to top](#table-of-contents)\n",
    "\n",
    "Use RagAgent to answer a multi-hop question.\n",
    "\n",
    "Problem: The common authors of the paper FLAML and paper AutoGen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muserproxy\u001b[0m (to rag):\n",
      "\n",
      "The common authors of the paper FLAML and paper AutoGen.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 04:26:56,404 - utils.py:   16 - DEBUG - is_code_execution_result: False\u001b[0m\n",
      "2024-02-26 04:26:56,404 - utils.py:   16 - DEBUG - \u001b[32mInput message: The common authors of the paper FLAML and paper AutoGen.\u001b[0m\n",
      "2024-02-26 04:26:56,405 - utils.py:   16 - DEBUG - \u001b[32mReceived raw message: The common authors of the paper FLAML and paper AutoGen.\u001b[0m\n",
      "2024-02-26 04:26:56,406 - utils.py:   16 - DEBUG - \u001b[32mPerforming RAG for message: The common authors of the paper FLAML and paper AutoGen.\u001b[0m\n",
      "2024-02-26 04:26:56,432 - utils.py:   16 - DEBUG - \u001b[32mTask predicted: multihop\u001b[0m\n",
      "2024-02-26 04:26:56,432 - utils.py:   16 - DEBUG - __call__ took 0.03 seconds.\u001b[0m\n",
      "2024-02-26 04:26:56,433 - utils.py:   16 - DEBUG - \u001b[32mRefined message for db query: ['Who are the authors of the paper FLAML', 'Who are the authors of the paper AutoGen', 'Are there any authors common to both papers']\u001b[0m\n",
      "2024-02-26 04:26:56,502 - utils.py:   16 - DEBUG - retrieve_docs took 0.07 seconds.\u001b[0m\n",
      "2024-02-26 04:26:56,505 - utils.py:   16 - DEBUG - get_docs_by_ids took 0.00 seconds.\u001b[0m\n",
      "2024-02-26 04:26:56,506 - utils.py:   16 - DEBUG - rerank took 0.00 seconds.\u001b[0m\n",
      "2024-02-26 04:26:56,508 - utils.py:   16 - DEBUG - get_docs_by_ids took 0.00 seconds.\u001b[0m\n",
      "2024-02-26 04:26:56,509 - utils.py:   16 - DEBUG - convert_get_results_to_query_results took 0.00 seconds.\u001b[0m\n",
      "2024-02-26 04:26:56,510 - utils.py:   16 - DEBUG - perform_rag took 0.10 seconds.\u001b[0m\n",
      "2024-02-26 04:26:56,510 - utils.py:   16 - DEBUG - \u001b[32mUsed doc ids: set()\u001b[0m\n",
      "2024-02-26 04:26:56,512 - utils.py:   16 - DEBUG - \u001b[32mAdding doc_id 5674436919760705058 to context.\u001b[0m\n",
      "2024-02-26 04:26:56,513 - utils.py:   16 - DEBUG - \u001b[32mAdding doc_id 555955145001710383 to context.\u001b[0m\n",
      "2024-02-26 04:26:56,514 - utils.py:   16 - DEBUG - \u001b[32mAdding doc_id 2404376237972229059 to context.\u001b[0m\n",
      "2024-02-26 04:26:56,515 - utils.py:   16 - DEBUG - \u001b[32mAdding doc_id 5588329763957448014 to context.\u001b[0m\n",
      "2024-02-26 04:26:56,516 - utils.py:   16 - DEBUG - process_message took 0.11 seconds.\u001b[0m\n",
      "2024-02-26 04:26:56,517 - utils.py:   16 - DEBUG - \u001b[32mTo LLM message: You're a helpful AI assistant with retrieval augmented generation capability.\n",
      "You answer user's ques\u001b[0m\n",
      "2024-02-26 04:26:56,517 - utils.py:   16 - DEBUG - \u001b[32mTokens in history: 0\u001b[0m\n",
      "2024-02-26 04:26:56,546 - utils.py:   16 - DEBUG - Inner LLM reply: {'content': 'The common authors of the paper FLAML and paper AutoGen are Qingyun Wu, Erkang Zhu, and Chi Wang.', 'role': 'user'}\u001b[0m\n",
      "2024-02-26 04:26:56,550 - utils.py:   16 - DEBUG - Inner proxy reply: \u001b[0m\n",
      "2024-02-26 04:26:56,550 - utils.py:   16 - DEBUG - Is update context: False, new message: The common authors of the paper FLAML and paper AutoGen are Qingyun Wu, Erkang Zhu, and Chi Wang., length: 97\u001b[0m\n",
      "2024-02-26 04:26:56,551 - utils.py:   16 - DEBUG - process_message took 0.00 seconds.\u001b[0m\n",
      "2024-02-26 04:26:56,551 - utils.py:   16 - DEBUG - \u001b[32mTo LLM message: \u001b[0m\n",
      "2024-02-26 04:26:56,551 - utils.py:   16 - DEBUG - \u001b[32mTokens in history: 52\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mrag\u001b[0m (to userproxy):\n",
      "\n",
      "The common authors of the paper FLAML and paper AutoGen are Qingyun Wu, Erkang Zhu, and Chi Wang.\n",
      "\n",
      "Source: /datadrive/autogen/notebook/../website/docs/Ecosystem.md ./tmp/download/Research.md /datadrive/autogen/notebook/../website/docs/Research.md\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muserproxy\u001b[0m (to rag):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 04:26:56,555 - utils.py:   16 - DEBUG - History message: {'content': 'The common authors of the paper FLAML and paper AutoGen.', 'role': 'user'}\u001b[0m\n",
      "2024-02-26 04:26:56,555 - utils.py:   16 - DEBUG - History message: {'content': 'The common authors of the paper FLAML and paper AutoGen are Qingyun Wu, Erkang Zhu, and Chi Wang.\\n\\nSource: /datadrive/autogen/notebook/../website/docs/Ecosystem.md ./tmp/download/Research.md /datadrive/autogen/notebook/../website/docs/Research.md', 'role': 'assistant'}\u001b[0m\n",
      "2024-02-26 04:26:56,555 - utils.py:   16 - DEBUG - History message: {'content': 'The common authors of the paper FLAML and paper AutoGen are Qingyun Wu, Erkang Zhu, and Chi Wang.\\n\\nSource: /datadrive/autogen/notebook/../website/docs/Ecosystem.md ./tmp/download/Research.md /datadrive/autogen/notebook/../website/docs/Research.md', 'role': 'assistant'}\u001b[0m\n",
      "2024-02-26 04:26:56,556 - utils.py:   16 - DEBUG - is_code_execution_result: False\u001b[0m\n",
      "2024-02-26 04:26:56,556 - utils.py:   16 - DEBUG - \u001b[32mInput message: \u001b[0m\n",
      "2024-02-26 04:26:56,557 - utils.py:   16 - DEBUG - \u001b[32mReceived raw message: The common authors of the paper FLAML and paper AutoGen.\u001b[0m\n",
      "2024-02-26 04:26:56,557 - utils.py:   16 - DEBUG - Is update context: False, new message: , length: 0\u001b[0m\n",
      "2024-02-26 04:26:56,558 - utils.py:   16 - DEBUG - process_message took 0.00 seconds.\u001b[0m\n",
      "2024-02-26 04:26:56,558 - utils.py:   16 - DEBUG - \u001b[32mTo LLM message: \u001b[0m\n",
      "2024-02-26 04:26:56,558 - utils.py:   16 - DEBUG - \u001b[32mTokens in history: 79\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# reset the assistant. Always reset the assistant before starting a new conversation.\n",
    "rag.reset()\n",
    "\n",
    "qa_problem = \"The common authors of the paper FLAML and paper AutoGen.\"\n",
    "rag.rag_filter_document = None  # clear filter\n",
    "_ = userproxy.initiate_chat(rag, message=qa_problem)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
