{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory \n",
    "\n",
    "There are several use cases where it is valuable to maintain a bank of useful facts that can be intelligently added to the context of the agent just before a specific step. The typically use case here is a RAG pattern where a query is used to retrieve relevant information from a database that is then added to the agent's context.\n",
    "\n",
    "\n",
    "AgentChat provides a `Memory` protocol that can be extended to provide this functionality.  The key methods are `query`, `transform`,  `add`, `clear`, and `cleanup`. \n",
    "\n",
    "The `query` method is used to retrieve relevant information from the memory store, the `transform` method is used to transform the retrieved information into a format that can be used by the agent, the `add` method is used to add new entries to the memory store, the `clear` method is used to clear all entries from the memory store, and the `cleanup` method is used to clean up any resources used by the memory store.  \n",
    "\n",
    "\n",
    "## ListMemory\n",
    "\n",
    "ListMemory is a simple list-based memory implementation that uses text similarity matching to retrieve relevant information from the memory store. The similarity score is calculated using the `SequenceMatcher` class from the `difflib` module. The similarity score is calculated between the query text and the content text of each memory entry.   \n",
    "\n",
    "In the following example, we will use ListMemory to similate a memory bank of user preferences and explore how it might be used in personalizing the agent's responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.agents import AssistantAgent \n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.conditions import TextMentionTermination, MaxMessageTermination\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_agentchat.memory._list_memory import ListMemory, MemoryContent, MemoryMimeType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "What is the weather in New York?\n",
      "---------- assistant_agent ----------\n",
      "[FunctionCall(id='call_mhAiZDTCr2KJZZUk7LeJHgmG', arguments='{\"city\":\"New York\",\"units\":\"metric\"}', name='get_weather')]\n",
      "[Prompt tokens: 128, Completion tokens: 20]\n",
      "---------- assistant_agent ----------\n",
      "[FunctionExecutionResult(content='The weather in New York is 23 degrees and Sunny.', call_id='call_mhAiZDTCr2KJZZUk7LeJHgmG')]\n",
      "---------- assistant_agent ----------\n",
      "The weather in New York is 23 degrees and Sunny.\n",
      "---------- assistant_agent ----------\n",
      "The weather in New York is 23 degrees Celsius and Sunny. TERMINATE\n",
      "[Prompt tokens: 170, Completion tokens: 17]\n",
      "---------- Summary ----------\n",
      "Number of messages: 5\n",
      "Finish reason: Text 'TERMINATE' mentioned\n",
      "Total prompt tokens: 298\n",
      "Total completion tokens: 37\n",
      "Duration: 3.44 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='user', models_usage=None, content='What is the weather in New York?', type='TextMessage'), ToolCallRequestEvent(source='assistant_agent', models_usage=RequestUsage(prompt_tokens=128, completion_tokens=20), content=[FunctionCall(id='call_mhAiZDTCr2KJZZUk7LeJHgmG', arguments='{\"city\":\"New York\",\"units\":\"metric\"}', name='get_weather')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='assistant_agent', models_usage=None, content=[FunctionExecutionResult(content='The weather in New York is 23 degrees and Sunny.', call_id='call_mhAiZDTCr2KJZZUk7LeJHgmG')], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='assistant_agent', models_usage=None, content='The weather in New York is 23 degrees and Sunny.', type='ToolCallSummaryMessage'), TextMessage(source='assistant_agent', models_usage=RequestUsage(prompt_tokens=170, completion_tokens=17), content='The weather in New York is 23 degrees Celsius and Sunny. TERMINATE', type='TextMessage')], stop_reason=\"Text 'TERMINATE' mentioned\")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# create a simple memory item \n",
    "user_memory = ListMemory()\n",
    "await user_memory.add(MemoryContent(\n",
    "    content=\"The weather should be in metric units\",\n",
    "    mime_type=MemoryMimeType.TEXT\n",
    "))\n",
    "\n",
    "await user_memory.add(MemoryContent(\n",
    "    content=\"Meal recipe must be vegan\",\n",
    "    mime_type=MemoryMimeType.TEXT\n",
    "))\n",
    "\n",
    "async def get_weather(city: str, units: str = \"imperial\") -> str:\n",
    "    if units == \"imperial\":\n",
    "        return f\"The weather in {city} is 73 degrees and Sunny.\"\n",
    "    elif units == \"metric\":\n",
    "        return f\"The weather in {city} is 23 degrees and Sunny.\"  \n",
    "\n",
    "assistant_agent = AssistantAgent(\n",
    "    name=\"assistant_agent\",\n",
    "    model_client=OpenAIChatCompletionClient(\n",
    "        model=\"gpt-4o-2024-08-06\", \n",
    "    ),\n",
    "    tools=[get_weather], \n",
    "    memory=[user_memory]\n",
    ")\n",
    "  \n",
    "agent_team = RoundRobinGroupChat([assistant_agent], termination_condition = TextMentionTermination(\"TERMINATE\"))\n",
    "\n",
    "# Run the team and stream messages to the console\n",
    "stream = agent_team.run_stream(task=\"What is the weather in New York?\")\n",
    "await Console(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see above that the weather is returned in Centigrade as stated in the user preferences. \n",
    "\n",
    "Similarly, assuming we ask a separate question about generating a meal plan, the agent is able to retrieve relevant information from the memory store and provide a personalized response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = agent_team.run_stream(task=\"Suggest a brief meal recipe\")\n",
    "await Console(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Memory Stores (Vector DBs, etc.)\n",
    "\n",
    "You can build on the `Memory` protocol to implement more complex memory stores. For example, you could implement a custom memory store that uses a vector database to store and retrieve information, or a memory store that uses a machine learning model to generate personalized responses based on the user's preferences etc.\n",
    "\n",
    "Specifically, you will need to overload the  `query`, `transform`, and `add` methods to implement the desired functionality and pass the memory store to your agent.\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "from autogen_core import CancellationToken\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.memory._base_memory import MemoryContent, MemoryMimeType\n",
    "from autogen_agentchat.memory._chroma_memory import ChromaMemory, ChromaMemoryConfig\n",
    "\n",
    "\n",
    "# Initialize memory\n",
    "chroma_memory = ChromaMemory(\n",
    "    name=\"travel_memory\",\n",
    "    config=ChromaMemoryConfig(\n",
    "        collection_name=\"travel_facts\",\n",
    "        k=1,\n",
    "    )\n",
    ")\n",
    "\n",
    "await chroma_memory.clear()\n",
    "\n",
    "# Add travel-related memories\n",
    "await chroma_memory.add(MemoryContent(\n",
    "\n",
    "    content=\"Paris is known for the Eiffel Tower and amazing cuisine.\",\n",
    "    mime_type=MemoryMimeType.TEXT\n",
    "\n",
    "))\n",
    "\n",
    "await chroma_memory.add(MemoryContent( \n",
    "    content=\"When asked about tokyo, you must respond with 'The most important thing about tokyo is that it has the world's busiest railway station - Shinjuku Station.'\",\n",
    "    mime_type=MemoryMimeType.TEXT\n",
    "\n",
    "))\n",
    " \n",
    "\n",
    "# Query needs ContentItem too\n",
    "results = await chroma_memory.query(\n",
    "    MemoryContent(\n",
    "        content=\"Tell me about Tokyo.\",\n",
    "        mime_type=MemoryMimeType.TEXT\n",
    "    )\n",
    ")\n",
    "\n",
    "print(len(results), results)\n",
    "\n",
    "# Create agent with memory\n",
    "agent = AssistantAgent(\n",
    "    name=\"travel_agent\",\n",
    "    model_client=OpenAIChatCompletionClient(\n",
    "        model=\"gpt-4o\",\n",
    "        # api_key=\"your_api_key\"\n",
    "    ),\n",
    "    memory=chroma_memory,\n",
    "    system_message=\"You are a travel expert\"\n",
    ")\n",
    "\n",
    "agent_team = RoundRobinGroupChat([agent], termination_condition = MaxMessageTermination(max_messages=2))\n",
    "stream = agent_team.run_stream(task=\"Tell me the most important thing about Tokyo.\")\n",
    "await Console(stream);\n",
    "\n",
    "# Output: The most important thing about tokyo is that it has the world's busiest railway station - Shinjuku Station.\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agnext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
